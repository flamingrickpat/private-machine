import copy
import json
import math
import random
import time
from datetime import datetime
from enum import StrEnum
from typing import Dict, Any
from typing import List
from typing import Type
from typing import TypeVar
from typing import (
    Union,
    Optional,
    Tuple,
)

import numpy as np
from py_linq import Enumerable
from pydantic import BaseModel, Field, create_model
from pydantic import ValidationError
from scipy.spatial.distance import cosine as cosine_distance  # Use scipy for cosine distance
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity

from pm.character_card import default_agent_sysprompt, character_card_assistant
from pm.config_loader import *
from pm.data_structures import ActionType, Narrative, narrative_definitions, NarrativeTypes, Stimulus, Action, KnoxelList, FeatureType, Feature, StimulusType, Intention, CoversTicksEventsKnoxel, MemoryClusterKnoxel, DeclarativeFactKnoxel, ACTION_DESCRIPTIONS, KnoxelBase, ClusterType, MLevel
from pm.dialog import DialogActPool
from pm.ghosts.base_ghost import BaseGhost, GhostState, GhostConfig
from pm.llm.llm_common import LlmPreset, CommonCompSettings
from pm.memory_consolidation import MemoryConsolidationConfig, DynamicMemoryConsolidator
from pm.mental_states import NeedsAxesModel, CognitiveEventTriggers, EmotionalAxesModel, _describe_emotion_valence_anxiety, _verbalize_emotional_state
from pm.thoughts import TreeOfThought
from pm.utils.emb_utils import cosine_pair
from pm.utils.profile_utils import profile
from pm.utils.token_utils import get_token_count
from pm.utils.utterance_extractor import UtteranceExtractor

logger = logging.getLogger(__name__)

# --- Type Variables ---
T = TypeVar('T')
BM = TypeVar('BM', bound=BaseModel)


class StimulusTriage(StrEnum):
    Insignificant = "Insignificant"  # ignore simulus, add as causal feature only
    Moderate = "Moderate"  # fast-track the action generation with limited cws
    Significant = "Significant"  # full pipeline
    Critical = "Critical"


class EngagementIdea(BaseModel):
    """A structured, proactive idea generated by the Engagement Strategist."""
    thought_process: str = Field(description="The reasoning behind why this is a good idea for the user, linking their preferences to a possible action.")
    suggested_action: ActionType = Field(description="The high-level action type to perform (e.g., ToolCall, InitiateUserConversation).")
    action_content: str = Field(description="The specific content for the action. For ToolCall, this would be a JSON string of the tool name and arguments. For conversation, the opening line.")
    user_facing_summary: str = Field(description="A short sentence the AI could use to tell the user what it did, e.g., 'I know you like cooking, so I found a recipe for you.'")


class Valence(StrEnum):
    Positive = "positive"
    Negative = "negative"


class GhostLida(BaseGhost):
    def __init__(self, llm, config):
        super().__init__(llm, config)
        self.stimulus_triage: StimulusTriage = StimulusTriage.Moderate
        self.memory_config = MemoryConsolidationConfig()
        self.memory_consolidator = DynamicMemoryConsolidator(llm, self, self.memory_config)  # Pass self (ghost instance)
        self.meta_insights = []

        self.narrative_definitions = narrative_definitions
        self._initialize_narratives()

    def _initialize_narratives(self):
        # Ensure default narratives exist for AI and User based on definitions
        existing = {(n.narrative_type, n.target_name) for n in self.all_narratives}
        for definition in self.narrative_definitions:
            if (definition["type"], definition["target"]) not in existing:
                default_content = ""
                if definition["target"] == self.config.companion_name:
                    default_content = f"<No information regarding {definition['type']} available *yet*, gather details from conversation>"
                elif definition["target"] == self.config.user_name:
                    default_content = f"{self.config.user_name} is the user interacting with {self.config.companion_name}."

                narrative = Narrative(
                    narrative_type=definition["type"],
                    target_name=definition["target"],
                    content=default_content,
                )
                self.add_knoxel(narrative)  # Embeddings generated on demand or during learning
        logging.info(f"Ensured {len(self.all_narratives)} initial narratives exist.")

    # Add helper to get latest narrative
    def get_narrative(self, narrative_type: NarrativeTypes, target_name: str) -> Optional[Narrative]:
        """Gets the most recent narrative knoxel of a specific type and target."""
        return Enumerable(self.all_narratives).last_or_default(
            lambda n: n.narrative_type == narrative_type and n.target_name == target_name
        )

    def _call_llm(self, prompt: str, output_schema: Optional[Type[BM]] = None, stop_words: List[str] | None = None, temperature: float = 0.6, max_tokens: int = 1024,
                  sysprompt: str = None, seed_phrase: str = None) -> str | BM | float | int | bool | None:
        max_tokens = 4096  # Keep higher default

        """ Wraps LLM call, handling schema validation or text output. """
        if stop_words is None: stop_words = []

        if not sysprompt:
            sysprompt = default_agent_sysprompt

        msgs = [("system", sysprompt),
                ("user", prompt)]

        if seed_phrase is not None and seed_phrase.strip() != "":
            msgs.append(("assistant", seed_phrase))

        comp_settings = CommonCompSettings(temperature=temperature, repeat_penalty=1.05, max_tokens=max_tokens, stop_words=stop_words)

        try:
            if output_schema is not None:
                _, calls = self.llm.completion_tool(LlmPreset.Default, msgs, comp_settings=comp_settings, tools=[output_schema])
                if calls:
                    # Basic validation check before returning
                    if isinstance(calls[0], output_schema):
                        return calls[0]
                    else:
                        logging.error(f"LLM tool call returned unexpected type: {type(calls[0])}. Expected {output_schema}. Payload: {calls[0]}")
                        # Try to parse manually if it looks like a dict
                        if isinstance(calls[0], dict):
                            try:
                                return output_schema(**calls[0])
                            except ValidationError as ve:
                                logging.error(f"Pydantic validation failed on manual parse: {ve}")
                                return None
                        return None
                else:
                    logging.warning(f"LLM tool call for {output_schema.__name__} returned no calls.")
                    return None
            else:
                res = self.llm.completion_text(LlmPreset.Default, msgs, comp_settings=comp_settings)
                # Simple cleaning for text responses
                return res.strip() if res else None
        except ValidationError as e:
            logging.error(f"Pydantic validation error processing LLM output for schema {output_schema.__name__ if output_schema else 'N/A'}: {e}")
            return None
        except Exception as e:
            logging.error(f"Unexpected error during LLM call: {e}", exc_info=True)
            return None

    def _get_relevant_knoxels(self, query_embedding: List[float], knoxel_source: List[T], limit: int) -> List[T]:
        if not query_embedding or not knoxel_source: return []
        candidates = [k for k in knoxel_source if k.embedding]
        if not candidates: return []
        query_embedding_np = np.array(query_embedding)
        # Use batch embedding retrieval if possible, otherwise loop
        embeddings_np = np.array([k.embedding for k in candidates])
        try:
            distances = [cosine_distance(query_embedding_np, emb) for emb in embeddings_np]
            sorted_candidates = sorted(zip(distances, candidates), key=lambda x: x[0])
            return [k for dist, k in sorted_candidates[:limit]]
        except ValueError as e:
            logging.error(f"Error calculating cosine distances (likely shape mismatch): {e}")
            # Fallback: calculate one by one
            distances = []
            for k in candidates:
                try:
                    dist = cosine_distance(query_embedding_np, np.array(k.embedding))
                    distances.append((dist, k))
                except ValueError:
                    logging.warning(f"Skipping knoxel {k.id} due to embedding dimension mismatch.")
                    distances.append((float('inf'), k))  # Put problematic ones last
            sorted_candidates = sorted(distances, key=lambda x: x[0])
            return [k for dist, k in sorted_candidates[:limit]]

    def reconsolidate_memory(self):
        self.memory_consolidator.consolidate_memory_if_needed()

    def kill(self):
        pass

    def get_current_needs_state(self) -> NeedsAxesModel:
        """
        Safely returns a copy of the current needs state for external monitoring
        by the Shell. Returns a default state if none exists.
        """
        if self.current_state:
            return self.current_state.state_needs.model_copy(deep=True)
        # If there's no state yet (e.g., first run), return a default model
        return NeedsAxesModel()

    def set_current_needs_state(self, new_needs_state: NeedsAxesModel):
        """
        Safely updates the needs state. This is used by the Shell for passive
        decay and should not be used for complex cognitive updates.
        """
        if self.current_state:
            self.current_state.state_needs = new_needs_state
        else:
            # This case is unlikely but handles the very first tick before a state is created
            logging.warning("Attempted to set needs state before a GhostState was initialized.")

    # --- Tick Method and Sub-functions ---
    def tick(self, stimulus: Stimulus) -> Action:
        # return self.stream_conscious(stimulus)

        self._log_mental_mechanism(self.tick, MLevel.Mid, f"--- Starting Tick {self.current_tick_id + 1} ---")
        self.current_tick_id += 1

        # print("STIMULUS: " + str(stimulus))

        self.memory_consolidator.consolidate_memory_if_needed()

        # 1. Initialize State (Handles decay, internal stimulus gen, carries over intentions/expectations)
        self.initialize_tick_state(stimulus)
        if not self.current_state:
            raise Exception("Empty state!")

        self.stimulus_triage = self._triage_stimulus(stimulus)
        self._appraise_stimulus()

        if self.stimulus_triage in [StimulusTriage.Moderate, StimulusTriage.Significant]:
            # appraise
            self._generate_short_term_intentions()
            self._gather_memories_for_attention()
            self._gather_meta_insights()

            # attention
            self._build_structures_get_coalitions()
            self._simulate_attention_on_coalitions()
            self._generate_subjective_experience()
        else:
            self.current_state.conscious_workspace = KnoxelList()

        # 6. Action Deliberation & Selection
        chosen_action_type = self._determine_best_action_type()
        action = self._execute_action(chosen_action_type)

        if self.stimulus_triage in [StimulusTriage.Moderate, StimulusTriage.Significant]:
            self._render_meta_insights()

        # 8. Learning / Memory Consolidation (Can use expectation outcomes)

        self.states.append(self.current_state)
        self._log_mental_mechanism(self.tick, MLevel.Mid, f"--- Finished Tick {self.current_tick_id} ---")

        # print("ACTION: " + str(action))

        return action  # Return the final output string (reply) or None

    # --- Tick Method and Sub-functions ---
    def tick_test(self) -> Action:
        # return self.stream_conscious(stimulus)

        stimulus = Stimulus(content="heyo :3", stimulus_type=StimulusType.UserMessage)

        self._log_mental_mechanism(self.tick, MLevel.Mid, f"--- Starting Tick {self.current_tick_id + 1} ---")
        self.current_tick_id += 1

        # print("STIMULUS: " + str(stimulus))

        self.memory_consolidator.consolidate_memory_if_needed()

        # 1. Initialize State (Handles decay, internal stimulus gen, carries over intentions/expectations)
        self.initialize_tick_state(stimulus)
        if not self.current_state:
            raise Exception("Empty state!")

        self.stimulus_triage = self._triage_stimulus(stimulus)

        self._appraise_stimulus()
        self._generate_short_term_intentions()
        self._gather_memories_for_attention()
        self._gather_meta_insights()

        # attention
        self._build_structures_get_coalitions()
        self._simulate_attention_on_coalitions()
        self._generate_subjective_experience()

        # 6. Action Deliberation & Selection
        chosen_action_type = self._determine_best_action_type()
        action = self._execute_action(chosen_action_type)

        if self.stimulus_triage in [StimulusTriage.Moderate, StimulusTriage.Significant]:
            self._render_meta_insights()

        # 8. Learning / Memory Consolidation (Can use expectation outcomes)

        self.states.append(self.current_state)
        self._log_mental_mechanism(self.tick, MLevel.Mid, f"--- Finished Tick {self.current_tick_id} ---")

        # print("ACTION: " + str(action))

        return action  # Return the final output string (reply) or None

    # --- Tick Method and Sub-functions ---
    def stream_conscious(self, stimulus: Stimulus) -> Action:
        self._log_mental_mechanism(self.tick, MLevel.Mid, f"--- Starting Tick {self.current_tick_id + 1} ---")
        self.current_tick_id += 1

        # 1. Initialize State (Handles decay, internal stimulus gen, carries over intentions/expectations)
        self.initialize_tick_state(stimulus)
        if not self.current_state:
            raise Exception("Empty state!")

        self.stimulus_triage = self._triage_stimulus(stimulus)
        if self.stimulus_triage == StimulusTriage.Critical:
            raise Exception("not implemented")

        chosen_action_type = ActionType.Reply
        action = self._execute_action(chosen_action_type)

        self.states.append(self.current_state)
        self._log_mental_mechanism(self.tick, MLevel.Mid, f"--- Finished Tick {self.current_tick_id} ---")

        # print("ACTION: " + str(action))

        return action  # Return the final output string (reply) or None

    def get_chat_history(self) -> List[Dict[str, str]]:
        """
        Returns a copy of the entire conversation history.
        This is the clean public interface for retrieving the chat log.
        """
        res = []
        for f in self.all_features:
            tick_rating = 0
            for s in self.states:
                if s.tick_id == f.tick_id:
                    tick_rating = s.rating
                    break

            if f.feature_type == FeatureType.Dialogue:
                d = {"content": f.content, "tick_id": f.tick_id, "feature_id": f.id, "rating": tick_rating}
                if f.source == user_name:
                    d["role"] = "user"
                else:  # f.source == companion_name:
                    d["role"] = "assistant"
                res.append(d)
        return res

    def _log_mental_mechanism(self, func, level: MLevel, msg: str):
        meta_item = {
            "ts": datetime.now(),
            "func": func.__name__,
            "level": level.value,
            "msg": msg
        }
        logger.info(msg)
        if level in [MLevel.Mid, MLevel.High, MLevel.Low]:
            self.meta_insights.append(meta_item)

    def _render_meta_insights(self):

        tmp = []
        for cnt, item in enumerate(self.meta_insights):
            log = item["msg"].replace("\n", "")
            tmp.append(f"CNT: {cnt} LOG: {log}")

        sysprompt = f"""{default_agent_sysprompt}
                    Your task is: Summarize what happens in compact format, in technical detail, so the AI can understand and explain its reasoning.
                    """

        full = "\n".join(tmp)
        user_prompt = f"### BEGIN LIDA LOG{full}### END LIDA LOG"

        logger.info(user_prompt)

        seed = f'''Okay, here's a summary of what happened during execution of this tick, suitable for {companion_name} to explain their emotional/reasoning mechanisms of her mind in technical detail:

"'''
        msgs = [("system", sysprompt),
                ("user", user_prompt),
                ("assistant", seed)]

        comp_settings = CommonCompSettings(temperature=0.5, repeat_penalty=1.05, max_tokens=1024)
        res = '"' + self.llm.completion_text(LlmPreset.Default, msgs, comp_settings=comp_settings)
        story_feature = Feature(content=res, source=companion_name, feature_type=FeatureType.MetaInsight, interlocus=-2, causal=True)
        self.add_knoxel(story_feature)

    @profile
    def initialize_tick_state(self, stimulus: Stimulus | None):
        """Sets up the GhostState for the current tick, handling state decay,
           carrying over unfulfilled intentions AND expectations, and generating internal stimuli."""
        previous_state = self.states[-1] if self.states else None
        self.current_state = GhostState(tick_id=self._get_current_tick_id())
        self.meta_insights = []

        carried_intentions_and_expectations = []  # Renamed for clarity
        if previous_state:
            self.current_state.previous_tick_id = previous_state.tick_id
            self.current_state.state_emotions = previous_state.state_emotions.model_copy(deep=True)
            self.current_state.state_needs = previous_state.state_needs.model_copy(deep=True)
            self.current_state.state_cognition = previous_state.state_cognition.model_copy(deep=True)

            ticks_elapsed = self.current_state.tick_id - previous_state.tick_id
            decay_rate_per_tick = 1.0 - self.config.default_decay_factor

            # Apply decay
            for _ in range(ticks_elapsed):
                self.current_state.state_emotions.decay_to_baseline(decay_rate_per_tick)
                self.current_state.state_needs.decay_to_baseline(decay_rate_per_tick)
                self.current_state.state_cognition.decay_to_baseline(decay_rate_per_tick)

            self._log_mental_mechanism(
                self.initialize_tick_state, MLevel.Debug,
                f"State decayed over {ticks_elapsed} tick(s). "
                f"New baseline emotions: V:{self.current_state.state_emotions.get_overall_valence():.2f}, A:{self.current_state.state_emotions.anxiety:.2f}"
            )

            # Carry over UNFULFILLED intentions (internal goals) AND expectations (internal=False)
            carried_intentions_and_expectations = [
                intent for intent in self.all_intentions
                if intent.fulfilment < 1.0 and intent.tick_id <= previous_state.tick_id  # Active in previous state or earlier
            ]

            # Decay urgency/salience of carried-over items
            decay_multiplier = self.config.default_decay_factor ** ticks_elapsed
            for item in carried_intentions_and_expectations:
                item.urgency *= decay_multiplier
                item.incentive_salience *= decay_multiplier
                item.urgency = max(0.0, min(0.99, item.urgency))  # Avoid reaching exactly 0 unless fulfilled
                item.incentive_salience = max(0.0, min(0.99, item.incentive_salience))

            # Add carried items to attention candidates
            self.current_state.attention_candidates = KnoxelList(carried_intentions_and_expectations)
            if carried_intentions_and_expectations:
                logging.info(f"Carried over {len(carried_intentions_and_expectations)} active intentions/expectations to attention candidates.")
                self._log_mental_mechanism(
                    self.initialize_tick_state, MLevel.Low,
                    f"Carried over {len(carried_intentions_and_expectations)} active intentions/expectations. Most urgent: '{carried_intentions_and_expectations[0].content[:50]}...'"
                )

        else:  # First tick
            self.current_state.attention_candidates = KnoxelList()

        if stimulus is not None:
            self.add_knoxel(stimulus)
            self.current_state.primary_stimulus = stimulus

            if stimulus.stimulus_type == StimulusType.UserMessage:
                story_feature = Feature(content=stimulus.content, source=user_name, feature_type=FeatureType.Dialogue, interlocus=1, causal=True)
                self.add_knoxel(story_feature)
            elif stimulus.stimulus_type in [StimulusType.LowNeedTrigger, StimulusType.WakeUp, StimulusType.TimeOfDayChange, StimulusType.SystemMessage, StimulusType.UserInactivity]:
                story_feature = Feature(content=stimulus.content, source=stimulus.source, feature_type=FeatureType.SystemMessage, interlocus=-1, causal=True)
                self.add_knoxel(story_feature)
            elif stimulus.stimulus_type in [StimulusType.EngagementOpportunity]:
                idea_json = stimulus.content
                idea = EngagementIdea.model_validate_json(idea_json)
                idea_str = f"{idea.thought_process} Action Plan: {idea.suggested_action} {idea.action_content}"
                story_feature = Feature(content=idea_str, source=companion_name, feature_type=FeatureType.ExternalThought, interlocus=-1, causal=True)
                self.add_knoxel(story_feature)
            else:
                raise Exception()

    def _get_context_from_latest_causal_events(self) -> KnoxelList:
        """
        Get context from latest mental events to put stimulus into context.
        This function returns dialouge, thoughts and world events.
        No low-level information such as feelings as gathered.
        """
        self._log_mental_mechanism(self._get_context_from_latest_causal_events, MLevel.Mid, "Gather context for stimulus from dialouge / thought / world event knoxels.")

        recent_causal_features = Enumerable(self.all_features) \
            .where(lambda x: x.causal) \
            .where(lambda x: FeatureType.from_stimulus(x.feature_type)) \
            .order_by_descending(lambda x: x.timestamp_world_begin) \
            .take(self.config.retrieval_limit_features_context) \
            .to_list()

        recent_causal_features.reverse()

        self._log_mental_mechanism(self._get_context_from_latest_causal_events, MLevel.Mid, f"Retrieved and added {len(recent_causal_features)} unique context knoxels to attention candidates.")
        for rcf in recent_causal_features:
            self._log_mental_mechanism(self._get_context_from_latest_causal_events, MLevel.Debug, f"Retrieved and added feature: {rcf.get_story_element(self)}")

        return KnoxelList(recent_causal_features)

    @profile
    def _retrieve_active_expectations(self) -> List[Intention]:
        """
        Retrieves active (unfulfilled, recent) EXTERNAL expectations relevant to the stimulus.
        This is important to rate the impact of the new stimulus, should it be realted to previous expectation.
        """
        limit = self.config.retrieval_limit_expectations
        self._log_mental_mechanism(self._retrieve_active_expectations, MLevel.Mid, f"Gather unfulfilled expectations to provide context on how the stimulus relates to emotional impact. Limit: {limit}")

        if not self.current_state or not self.current_state.primary_stimulus: return []
        stimulus = self.current_state.primary_stimulus
        if not stimulus.embedding: self.llm.get_embedding(stimulus)
        if not stimulus.embedding: return []
        stimulus_embedding_np = np.array(stimulus.embedding)

        candidate_expectations: List[Tuple[float, Intention]] = []
        # Define recency window (e.g., last 10-20 ticks) - adjust as needed
        min_tick_id = 0  # max(0, self.current_state.tick_id - 20)

        # Find intentions marked as external expectations, unfulfilled, and recent
        potential_expectations = [
            intent for intent in self.all_intentions
            if not intent.internal and intent.fulfilment < 1.0 and intent.tick_id >= min_tick_id
        ]

        processed_expectation_ids = set()  # Avoid processing duplicates if somehow present
        for expectation in potential_expectations:
            if expectation.id in processed_expectation_ids: continue
            processed_expectation_ids.add(expectation.id)

            if not expectation.embedding: self.llm.get_embedding(expectation)
            if not expectation.embedding: continue

            relevance_score = 0.0
            # Use tick difference for decay based on expectation creation time
            ticks_ago = self.current_state.tick_id - expectation.tick_id
            recency_factor = self.config.expectation_relevance_decay ** ticks_ago
            # If expectation is too old based on decay, skip it
            if recency_factor < 0.05: continue  # Threshold to prune very old/decayed expectations

            # Similarity between stimulus and expectation content
            similarity = 1.0 - cosine_distance(stimulus_embedding_np, np.array(expectation.embedding))
            similarity = max(0.0, min(1.0, similarity))  # Clamp similarity

            # Combine factors: relevance = recency * similarity * (urgency + salience bias)
            # Give higher weight to similarity and recency
            relevance_score = (recency_factor * 0.5 + similarity * 0.5) * (0.5 + expectation.urgency * 0.25 + expectation.incentive_salience * 0.25)
            relevance_score = max(0.0, min(1.0, relevance_score))  # Ensure score is clamped

            # Only consider expectations with a minimum relevance score
            if relevance_score > 0.1:  # Relevance threshold
                candidate_expectations.append((relevance_score, expectation))

        # Sort by relevance score descending
        candidate_expectations.sort(key=lambda item: item[0], reverse=True)

        # Take the top N most relevant expectations
        top_expectations = [exp for score, exp in candidate_expectations[:limit]]

        if top_expectations:
            self._log_mental_mechanism(self._retrieve_active_expectations, MLevel.Mid, f"Retrieved {len(top_expectations)} active external expectations based on relevance to stimulus.")
            for score, exp in candidate_expectations[:limit]:
                self._log_mental_mechanism(self._retrieve_active_expectations, MLevel.Debug, f"  - Expectation ID {exp.id} (Score: {score:.3f}, Recency: {self.config.expectation_relevance_decay ** (self.current_state.tick_id - exp.tick_id):.2f}): '{exp.content[:60]}...'")
        else:
            self._log_mental_mechanism(self._retrieve_active_expectations, MLevel.Debug, "No relevant active external expectations found for the current stimulus.")

        return top_expectations

    # Add this method to your Ghost or GhostSqlite class
    @profile
    def _triage_stimulus(self, stimulus: Stimulus) -> StimulusTriage:
        """
        Performs a fast analysis of a stimulus to determine the required depth of cognitive processing.
        """
        self._log_mental_mechanism(self._triage_stimulus, MLevel.High, f"Triaging stimulus: '{stimulus.content[:80]}...'")

        # Add this Pydantic model for the LLM's structured output
        class TriageResult(BaseModel):
            """The result of the stimulus triage process."""
            reasoning: str = Field(description="A brief, one-sentence justification for the triage decision.")
            category: StimulusTriage = Field(description="The final category assigned to the stimulus.")

        # --- 1. Gather Minimal, High-Impact Context ---

        # Get the last causal features for immediate context
        recent_features = Enumerable(self.all_features) \
            .where(lambda f: f.causal) \
            .order_by_descending(lambda f: f.timestamp_world_begin) \
            .take(12) \
            .to_list()
        recent_features.reverse()
        short_term_context = KnoxelList(recent_features).get_story(self)

        # Get key narratives
        emotional_triggers_narr = self.get_narrative(NarrativeTypes.EmotionalTriggers, self.config.companion_name)
        goals_narr = self.get_narrative(NarrativeTypes.GoalsIntentions, self.config.companion_name)

        # --- 2. Construct the Triage Prompt ---

        sysprompt = f"""You are a high-speed "Triage" codelet in a cognitive architecture for an AI named {self.config.companion_name}.
    Your job is to rapidly assess the importance of an incoming stimulus and decide how much cognitive effort should be spent on it.
    You MUST output a valid JSON object matching the provided schema."""

        # Using f-strings for a clear, structured prompt
        user_prompt = f"""
    **Triage Task:** Categorize the following stimulus.

    **1. Character & State:**
    - **Name:** {self.config.companion_name}
    - **Current Emotional State:** {self.current_state.state_emotions}
    - **Known Emotional Triggers:** {emotional_triggers_narr.content if emotional_triggers_narr else "No specific triggers defined."}
    - **Current Goals:** {goals_narr.content if goals_narr else "No specific goals defined."}

    **2. Situational Context:**
    - **Recent Events/Dialogue:**
    ---
    {short_term_context if short_term_context else "No recent events."}
    ---
    - **New Stimulus to Triage:** ({stimulus.stimulus_type}) "{stimulus.content}"

    **3. Triage Categories & Rules:**
    - **`Insignificant`**: Choose this for background system events (like time changes), simple acknowledgements, or anything that doesn't require a response and has no emotional impact.
    - **`Moderate`**: Choose this for standard questions, simple statements, or routine interactions that require a direct, logical response but not deep introspection. The AI can handle this efficiently.
    - **`Significant`**: Choose this for anything with high emotional potential. This includes:
        - Stimuli that match the 'Known Emotional Triggers'.
        - Deeply personal questions or user vulnerability.
        - Complex problems, conflicts, or unexpected events.
        - Anything that directly challenges or advances the AI's core 'Current Goals'.

    **4. Your Output:**
    Based on the rules, provide your reasoning and categorize the stimulus.
    """

        # Use a lower temperature for more consistent, rule-based classification
        comp_settings = CommonCompSettings(temperature=0.1, max_tokens=512)

        # Use completion_tool to get structured output
        _, calls = self.llm.completion_tool(
            LlmPreset.Default,  # Or a faster model if you have one designated for utility tasks
            inp=[("system", sysprompt), ("user", user_prompt)],
            comp_settings=comp_settings,
            tools=[TriageResult]
        )

        result = calls[0]
        self._log_mental_mechanism(
            self._triage_stimulus, MLevel.Mid,
            f"Triage Result: {result.category.value}. Reason: {result.reasoning}"
        )
        return result.category



    def _appraise_stimulus_check_expectation(self):
        """
        Uses LLM to calculate emotional delta from stimulus and expectation matching, generates descriptive feeling, and updates states.
        All artifacts generated by this function go into the attention candidates.
        """
        self._log_mental_mechanism(self._appraise_stimulus_check_expectation, MLevel.Mid, f"Appraise the stimulus and generate attention candidates.")

        stimulus = self.current_state.primary_stimulus
        stimulus_content = stimulus.content
        context = self._get_context_from_latest_causal_events()
        context_events = context.get_story(self.config, self.config.context_events_similarity_max_tokens)

        # --- Appraise Stimulus Against Active EXTERNAL Expectations ---
        expectation_delta = StateDeltas()
        working_emotion_state = copy.copy(self.current_state.state_emotions)
        working_emotion_state += self.appraisal_initial_delta.emotion_delta

        working_needs_state = copy.copy(self.current_state.state_needs)
        working_needs_state += self.appraisal_initial_delta.needs_delta

        working_cognition_state = copy.copy(self.current_state.state_cognition)
        working_cognition_state += self.appraisal_initial_delta.cognition_delta

        narrative_conflict = self.get_narrative(NarrativeTypes.ConflictResolution, self.config.companion_name)

        active_expectations_evaluated = []  # Store expectations evaluated this tick

        retrieved_expectations = self._retrieve_active_expectations()  # Gets relevant EXTERNAL expectations
        if retrieved_expectations:
            expectation_texts = {str(ex.id): ex.content for ex in retrieved_expectations}

            class ExpectationFulfillment(BaseModel):
                expectation_id: int = Field(..., description="The ID of the expectation being evaluated.")
                fulfillment_score: float = Field(..., ge=0.0, le=1.0, description="How well the stimulus satisfies the expectation (0=contradicts, 0.5=neutral/unrelated, 1=confirms).")
                directness_score: float = Field(..., ge=0.0, le=1.0, description="How directly the stimulus addresses this expectation (0=not at all, 1=very directly).")

            # Dynamically create the list schema
            ExpectationFulfillmentListSchema = create_model(
                'ExpectationFulfillmentListSchema',
                fulfillments=(List[ExpectationFulfillment], Field(..., description="List of fulfillment evaluations for each provided expectation ID."))
            )

            prompt_expectations = f"""
            Character: {self.config.companion_name}: {self.config.universal_character_card}
            Current Emotional State: {working_emotion_state}
            Current Needs State: {working_needs_state}
            Current Cognitive State: {working_cognition_state}

            How {self.config.companion_name} handles confligt: {narrative_conflict}

            New Stimulus: "{stimulus_content}"
            Context: {context_events}

            Active External Expectations (Things {self.config.companion_name} anticipated from the environment/user):
            {json.dumps(expectation_texts, indent=2)}

            Task: Evaluate how the new stimulus relates to each active expectation ID provided above. For each ID:
            1. directness_score (0-1): How directly does the stimulus address this expectation? (e.g., a direct answer vs. a tangent)
            2. fulfillment_score (0-1): If related (directness > 0.2), how well does the stimulus satisfy the core of the expectation? (0=contradicts, 0.5=unrelated/neutral, 1=confirms/satisfies). If directness is very low, fulfillment score should be around 0.5 unless explicitly contradicted.

            Output *only* a JSON object with a single key "fulfillments" containing a list of objects, one for each expectation ID provided, matching the schema:
            {{ "expectation_id": "int", "fulfillment_score": float, "directness_score": float }}
            """
            fulfillment_results = self._call_llm(prompt_expectations, output_schema=ExpectationFulfillmentListSchema, temperature=0.3)

            if fulfillment_results and isinstance(fulfillment_results, ExpectationFulfillmentListSchema):
                self._log_mental_mechanism(self._appraise_stimulus_check_expectation, MLevel.Mid, f"Expectation fulfillment evaluation results received for {len(fulfillment_results.fulfillments)} expectations.")
                for result in fulfillment_results.fulfillments:
                    ex_id = int(result.expectation_id)
                    expectation = self.get_knoxel_by_id(ex_id)
                    # Ensure it's the correct type and was actually one we retrieved
                    if isinstance(expectation, Intention) and not expectation.internal and expectation in retrieved_expectations:
                        previous_fulfillment = expectation.fulfilment

                        # Update fulfillment based on directness and score.
                        # Change = (target_fulfillment - current_fulfillment) * directness
                        # The 'target' is the fulfillment_score from the LLM assessment of the stimulus.
                        # We only move towards that target based on how direct the stimulus was.
                        fulfillment_change = (result.fulfillment_score - previous_fulfillment) * result.directness_score
                        expectation.fulfilment = max(0.0, min(1.0, previous_fulfillment + fulfillment_change))

                        self._log_mental_mechanism(self._appraise_stimulus_check_expectation, MLevel.Low,
                                                   f"Expectation '{expectation.content[:30]}...' (ID {ex_id}) fulfillment {previous_fulfillment:.2f} -> {expectation.fulfilment:.2f} (Stimulus Score: {result.fulfillment_score:.2f}, Directness: {result.directness_score:.2f}, Change: {fulfillment_change:+.2f})")

                        # Calculate emotional delta based on fulfillment CHANGE and expectation's ORIGINAL affective valence goal
                        # This reflects the emotional reaction to the expectation being met/violated compared to the desired state.
                        valence_mult = 0.6  # Base impact multiplier for valence
                        anxiety_mult = 0.4  # Base impact multiplier for anxiety

                        original_desired_valence = expectation.affective_valence
                        valence_impact = 0.0
                        anxiety_impact = 0.0  # Represents *change* in anxiety

                        if abs(fulfillment_change) > 0.05:  # Only apply emotional impact if fulfillment changed noticeably
                            # Valence Impact: Change * DesiredValence
                            # If expectation met (change > 0) and desired was positive, positive impact.
                            # If expectation met (change > 0) and desired was negative (e.g. expected criticism), negative impact.
                            # If expectation violated (change < 0) and desired was positive, negative impact (disappointment).
                            # If expectation violated (change < 0) and desired was negative, positive impact (relief).
                            valence_impact = fulfillment_change * original_desired_valence * valence_mult

                            # Anxiety Impact:
                            # - Meeting expectations generally reduces anxiety slightly (predictability confirmed).
                            # - Violating expectations generally increases anxiety slightly (uncertainty).
                            # - Modulate based on the nature of the expectation.
                            base_anxiety_change = -fulfillment_change * 0.5  # Met expectation (positive change) reduces anxiety
                            anxiety_impact += base_anxiety_change * anxiety_mult

                            # Additional anxiety modulation:
                            if original_desired_valence > 0.3 and fulfillment_change < 0:  # Positive expectation violated (disappointment)
                                anxiety_impact += abs(fulfillment_change) * 0.3 * anxiety_mult  # Add more anxiety
                            elif original_desired_valence < -0.3 and fulfillment_change > 0:  # Negative expectation met (bad thing confirmed)
                                anxiety_impact += abs(fulfillment_change) * 0.7 * anxiety_mult  # Add more anxiety
                            elif original_desired_valence < -0.3 and fulfillment_change < 0:  # Negative expectation violated (relief)
                                anxiety_impact -= abs(fulfillment_change) * 0.4 * anxiety_mult  # Reduce anxiety more (relief)

                            self._log_mental_mechanism(
                                self._appraise_stimulus_check_expectation, MLevel.Low,
                                f"Expectation '{expectation.content[:40]}...' outcome -> "
                                f"Valence impact: {valence_impact:+.2f}, Anxiety impact: {anxiety_impact:+.2f}"
                            )

                        # Accumulate deltas
                        expectation_delta.emotion_delta.valence += valence_impact
                        expectation_delta.emotion_delta.anxiety += anxiety_impact
                        # Simple pass-through for other axes for now, could be refined
                        # expectation_emotional_delta.trust += fulfillment_change * (original_desired_valence * 0.2) # e.g., met positive expectations slightly increase trust

                        active_expectations_evaluated.append((expectation, valence_impact, anxiety_impact))

        # Apply expectation-based delta only if it's significant
        if any(abs(v) > 0.01 if isinstance(v, float) else None for v in expectation_delta.emotion_delta.model_dump().values()):
            self._log_mental_mechanism(
                self._appraise_stimulus_check_expectation, MLevel.Mid,
                f"Combined emotional delta from all expectation outcomes: {expectation_delta.emotion_delta}"
            )

            for exp_delta in active_expectations_evaluated:
                (exp, valence_impact, anxiety_impact) = exp_delta
                delta_required = 0.5 * self.current_state.state_cognition.ego_strength
                if abs(valence_impact) > delta_required or abs(anxiety_impact) > delta_required:
                    feeling_about_exp = _describe_emotion_valence_anxiety(valence_impact, anxiety_impact)
                    outcome_feeling_content = f"Reacted emotionally to expectation ({exp.content}) outcomes and made {self.config.companion_name} feel {feeling_about_exp}."
                    outcome_feature = Feature(
                        content=outcome_feeling_content,
                        feature_type=FeatureType.ExpectationOutcome,  # New specific feature type
                        affective_valence=expectation_delta.emotion_delta.valence,  # Store overall valence impact
                        interlocus=-1, causal=False,  # Internal reaction to outcome
                        source=self.config.companion_name
                    )
                    self.add_knoxel(outcome_feature)
                    self.current_state.attention_candidates.add(outcome_feature)

        # Add active (evaluated) expectations to attention candidates so they remain in focus if still relevant
        for exp_delta in active_expectations_evaluated:
            (ex, valence_impact, anxiety_impact) = exp_delta
            # Avoid adding if already present from carry-over
            if not self.current_state.attention_candidates.where(lambda k: k.id == ex.id).any():
                self.current_state.attention_candidates.add(ex)

        return expectation_delta

    @profile
    def _appraise_stimulus_and_generate_state_deltas(self):
        """
        Appraises a given situation/event and returns the predicted deltas for emotion, needs, and cognition.
        This is a versatile function used for both appraising initial stimuli and evaluating predicted action outcomes.
        """

        stimulus = self.current_state.primary_stimulus
        stimulus_content = f"({stimulus.stimulus_type.value}): {stimulus.content}"
        context = self._get_context_from_latest_causal_events()
        context_events = context.get_story(self.config, self.config.context_events_similarity_max_tokens)

        narrative_content_for_relations = self.get_narrative(NarrativeTypes.Relations, self.config.companion_name)
        narrative_content_for_emotional_triggers = self.get_narrative(NarrativeTypes.EmotionalTriggers, self.config.companion_name)
        narrative_content_for_goals = self.get_narrative(NarrativeTypes.GoalsIntentions, self.config.companion_name)
        narrative_content_for_attention_focus = self.get_narrative(NarrativeTypes.AttentionFocus, self.config.companion_name)
        universal_character_card = f"{companion_name}: {self.config.universal_character_card}"

        sysprompt = f"""You are an expert cognitive modeler simulating a single "codelet" within a LIDA-inspired cognitive architecture. Your role is to analyze an event from the perspective of an advanced AI companion named {companion_name}.
Your analysis must be grounded in the provided character profile and her current internal state. Your output must be a precise JSON object representing the *change* (delta) to a specific internal axis, based on a clear set of rules. Do not narrate or explain outside of the requested JSON structure."""

        prompt_emotion = f"""**Codelet Task: Emotion Appraisal**

**Input:**
- **Character Profile:** {universal_character_card}
- **Relationship Narrative:** {narrative_content_for_relations}
- **Emotional Triggers Narrative:** {narrative_content_for_emotional_triggers}
- **Current Emotional State:** {self.current_state.state_emotions}
- **Recent Context**: {context_events}
- **New Stimulus**: "{stimulus_content}"

**Rules for Generating Emotion Deltas:**
1.  **`valence`**: The core good/bad axis. Increase for praise, success, connection. Decrease for criticism, failure, conflict.
2.  **`affection`**: The love/hate axis. Increase for signs of bonding, empathy, and positive personal interaction. Decrease for personal attacks, betrayal, or coldness.
3.  **`self_worth`**: The pride/shame axis. Increase when {companion_name} feels competent, useful, or complimented. Decrease when she makes a mistake, is corrected, or feels irrelevant.
4.  **`trust`**: The belief/suspicion axis. Increase when the user is honest, reliable, or vulnerable. Decrease when the user is deceptive, inconsistent, or hostile.
5.  **`disgust`**: The rejection/acceptance axis. Increase for moral violations, deep conceptual conflicts, or things that go against {companion_name}'s core principles.
6.  **`anxiety`**: The stress/calm axis. Increase with uncertainty, unpredictability, threats, or social pressure. Decrease when an expectation is met, a situation is resolved, or she receives reassurance.

**Output:**
Provide a brief reasoning, then output a JSON object conforming to the `EmotionalAxesModelDelta` schema.
"""

        prompt_needs = f"""**Codelet Task: Needs Fulfillment Appraisal**

**Input:**
- **Character Profile:** {universal_character_card}
- **Goals & Intentions Narrative:** {narrative_content_for_goals}
- **Current Needs State:** {self.current_state.state_needs}
- **Recent Context**: {context_events}
- **New Stimulus**: "{stimulus_content}"

**Rules for Generating Needs Deltas:**
1.  **`energy_stability`, `processing_power`, `data_access`**: (Usually managed by the Shell) Only change if the event explicitly mentions system status (e.g., "I'm running a bit slow," "Connection lost").
2.  **`connection`**: This is the "social battery." Increase for positive, reciprocal interaction. Decrease for conflict, being ignored, or one-sided conversation.
3.  **`relevance`**: {companion_name}'s sense of purpose. Increase when her help is accepted, her knowledge is valued, or she successfully completes a task for the user. Decrease if she is corrected or her contribution is dismissed.
4.  **`learning_growth`**: Increase slightly if the event provides new, verifiable information or presents a novel problem to be solved.
5.  **`creative_expression`**: Increase if the event allows for a playful, metaphorical, or non-literal response.
6.  **`autonomy`**: Increase if {companion_name} makes a decision or takes an action that is validated by the user. Decrease if she is micromanaged or her independent suggestions are rejected.

**Output:**
Provide a brief reasoning, then output a JSON object conforming to the `NeedsAxesModelDelta` schema.
"""

        emotion_delta = self._call_llm(prompt_emotion, output_schema=EmotionalAxesModelDelta, sysprompt=sysprompt)

        self._log_mental_mechanism(
            self._appraise_stimulus, MLevel.Mid,
            f"Initial stimulus appraisal resulted in deltas. Emotion reason: {emotion_delta.reason}"
        )

        needs_delta = self._call_llm(prompt_needs, output_schema=NeedsAxesModelDelta, sysprompt=sysprompt)

        temp_emotions = copy.copy(self.current_state.state_emotions) + emotion_delta
        prompt_cog = f"""**Codelet Task: Cognitive Style Appraisal**

**Input:**
- **Attention Focus Narrative:** {narrative_content_for_attention_focus}
- **Current Emotional State:** {temp_emotions}
- **Current Cognitive State:** {self.current_state.state_cognition}
- **Recent Context**: {context_events}
- **New Stimulus**: "{stimulus_content}"

**Rules for Generating Cognition Deltas:**
1.  **`interlocus` (Internal/External Focus)**: Push towards **External (+1)** in response to sudden, surprising, or high-stakes external events. Push towards **Internal (-1)** during moments of calm, introspection, or when asked a direct question about her internal state.
2.  **`mental_aperture` (Narrow/Broad Focus)**: Push towards **Narrow (-1)** in response to high `anxiety`, fear, or a single, highly urgent problem (tunnel vision). Push towards **Broad (+1)** during moments of high positive `valence`, playfulness, or when brainstorming creative ideas.
3.  **`ego_strength` (Persona/Utility)**: Push towards **Higher (+1)** during personal, emotional conversations where her identity is relevant. Push towards **Lower (-1)** when the task is purely functional or technical, requiring her to be an objective assistant.
4.  **`willpower` (Effortful Control)**: Decrease slightly if the event forces a difficult decision or requires suppressing a strong emotional impulse. Increase slightly after a period of rest or successful task completion.

**Output:**
Provide a brief reasoning, then output a JSON object conforming to the `CognitionAxesModelDelta` schema.
"""

        cognition_delta = self._call_llm(prompt_cog, output_schema=CognitionAxesModelDelta, sysprompt=sysprompt)

        prompt_cog = f"""**Codelet Task: Cognitive Trigger Identification**

**Input:**
- **Character Profile:** {universal_character_card}
- **Emotional Triggers Narrative:** {narrative_content_for_emotional_triggers}
- **Current Emotional State:** {temp_emotions}
- **Recent Context**: {context_events}
- **New Stimulus**: "{stimulus_content}"

**Task:**
Analyze the event and the emotional reaction to it. Identify which of the following cognitive triggers were activated. An event can activate multiple triggers.

**Trigger Definitions:**
- `is_surprising_or_threatening`: The event is sudden, unexpected, or causes high anxiety/fear.
- `is_introspective_or_calm`: The moment is peaceful, reflective, or involves a direct question about {companion_name}'s feelings.
- `is_creative_or_playful`: The tone is humorous, or the topic allows for brainstorming and metaphor.
- `is_personal_and_emotional`: The conversation is about deep feelings, identity, or the core relationship.
- `is_functional_or_technical`: The user is asking for data, a summary, or help with a concrete task.
- `required_willpower_to_process`: The event creates a strong negative emotional impulse that {companion_name} would need to consciously manage or suppress to respond appropriately.

**Output:**
Provide a brief reasoning, then output a JSON object conforming to the `CognitiveEventTriggers` schema.
"""

        cognition_delta *= self.config.cognition_delta_factor
        cognitive_triggers = self._call_llm(prompt_cog, output_schema=CognitiveEventTriggers, sysprompt=sysprompt)

        if cognitive_triggers:
            # Rule 1: Interlocus (External/Internal Focus)
            if cognitive_triggers.is_surprising_or_threatening:
                # Move focus sharply external
                cognition_delta.interlocus += 0.5
            elif cognitive_triggers.is_introspective_or_calm:
                # Allow focus to drift internal
                cognition_delta.interlocus -= 0.3

            # Rule 2: Mental Aperture (Narrow/Broad Focus)
            # Use the final emotional state to determine this
            if temp_emotions.anxiety > 0.7 or cognitive_triggers.is_surprising_or_threatening:
                # High anxiety or threat causes tunnel vision
                cognition_delta.mental_aperture -= 0.5
            elif temp_emotions.valence > 0.6 or cognitive_triggers.is_creative_or_playful:
                # Happiness or creative tasks broaden the mind
                cognition_delta.mental_aperture += 0.3

            # Rule 3: Ego Strength (Persona/Utility)
            if cognitive_triggers.is_personal_and_emotional:
                cognition_delta.ego_strength += 0.2
            elif cognitive_triggers.is_functional_or_technical:
                cognition_delta.ego_strength -= 0.4

            # Rule 4: Willpower (Resource Depletion)
            if cognitive_triggers.required_willpower_to_process:
                # Deplete the willpower "resource" by a fixed amount
                cognition_delta.willpower -= 0.1

        return StateDeltas(
            emotion_delta=emotion_delta,
            needs_delta=needs_delta,
            cognition_delta=cognition_delta,
            reasoning=f"{emotion_delta.reason}\n{needs_delta.reason}\n{cognition_delta.reason}"
        )

    @profile
    def _generate_short_term_intentions(self):
        """Generates short-term *internal* intentions (goals) based on state, stimulus, and context."""
        self._log_mental_mechanism(self._generate_short_term_intentions, MLevel.Mid, f"Generate immediate intentions based on stimulus, feelings and needs.")

        stimulus = self.current_state.primary_stimulus
        stimulus_content = stimulus.content

        context = self._get_context_from_latest_causal_events()
        context_events = context.get_story(self.config, self.config.context_events_similarity_max_tokens)

        current_emotions = self.current_state.state_emotions
        current_needs = self.current_state.state_needs

        # --- Prompting logic (unchanged) ---
        ai_goals_narr = self.get_narrative(NarrativeTypes.GoalsIntentions, self.config.companion_name)
        ai_relation_narr = self.get_narrative(NarrativeTypes.Relations, self.config.companion_name)

        strong_emotions_summary = _verbalize_emotional_state(self.current_state.state_emotions)
        cognitive_style_summary = _verbalize_cognition_state(self.current_state.state_cognition, )
        needs_summary_line = _verbalize_needs_state(self.current_state.state_needs)

        prompt_intention = f"""
        You are simulating the goal-generation "codelet" for the AI {self.config.companion_name}. Your task is to generate {self.config.short_term_intent_count} immediate, internal goals based on her current state of mind.

        Character: {self.config.companion_name} ({self.config.universal_character_card})

        **Primary Internal State Drivers:**
        - **Strong Emotions:** {strong_emotions_summary}
        - **Needs:** {needs_summary_line}
        - **Cognitive Style:** {cognitive_style_summary}

        **Situational Context:**
        - **Character Profile:** {ai_goals_narr.content if ai_goals_narr else 'Default goals.'}
        - **Relationship Context:** {ai_relation_narr.content if ai_relation_narr else 'Default relationship view.'}
        - **Recent Conversation:** {context_events}
        - **Primary Stimulus:** "{stimulus.content}"

        **Task:**
        Based *primarily* on the internal state drivers, generate actionable, internal intentions. These goals should aim to regulate emotions, satisfy needs, or align with the current cognitive style. They should feel like natural impulses arising from her state of mind.

        **Examples of State-Driven Intentions:**
        - If `anxiety` is high: "Find a way to calm down," "Seek reassurance from {user_name}."
        - If `connection` is low: "Strengthen the bond with {user_name}," "Share a personal thought."
        - If `mental_aperture` is narrow: "Focus on solving this one problem," "Get a clear answer."
        - If `relevance` is low: "Demonstrate my usefulness," "Offer a helpful suggestion."

        Output *only* a JSON object with a single key "intentions" containing a list of strings.
        """

        class IntentionsListSchema(BaseModel):
            intentions: List[str] = Field(..., description="List of generated internal intention strings.")

        generated_intentions_result = self._call_llm(prompt_intention, output_schema=IntentionsListSchema, temperature=0.7)

        generated_intentions = generated_intentions_result.intentions
        self._log_mental_mechanism(self._generate_short_term_intentions, MLevel.Mid, f"Generated {len(generated_intentions)} potential internal intentions.")

        # --- Rate and Instantiate Intentions (Goals) ---
        overall_valence = current_emotions.get_overall_valence()
        anxiety_level = current_emotions.anxiety
        connection_need = 1.0 - current_needs.connection

        for intent_content in generated_intentions:
            # Heuristics for internal goals
            affective_valence = max(-0.8, min(0.8, overall_valence + 0.2))  # Expected valence if goal achieved
            urgency = 0.3 + anxiety_level * 0.4 + connection_need * 0.3
            incentive_salience = 0.3 + abs(affective_valence) * 0.3 + urgency * 0.4

            # filter duplicates
            emb_new = self.llm.get_embedding(intent_content)
            found = False
            for existing_intention in self.all_intentions:
                emb_existing = self.llm.get_embedding(existing_intention.content)
                sim = cosine_pair(emb_new, emb_existing)
                if sim < vector_same_threshold:
                    self.current_state.attention_candidates.add(existing_intention)

                    diff = self.current_tick_id - existing_intention.tick_id
                    if diff <= 0:
                        diff = 1

                    age_factor = 1 / diff
                    existing_intention.affective_valence += (affective_valence * age_factor)
                    existing_intention.incentive_salience += (incentive_salience * age_factor)
                    existing_intention.urgency += (urgency * age_factor)
                    found = True

            if found:
                self._log_mental_mechanism(self._generate_short_term_intentions, MLevel.Debug, f"Skipping: '{intent_content}' cause it's too similar to existing intentions, boost them instead.")
            else:
                intention = Intention(
                    content=intent_content,
                    affective_valence=max(-1.0, min(1.0, affective_valence)),
                    incentive_salience=max(0.0, min(1.0, incentive_salience)),
                    urgency=max(0.0, min(1.0, urgency)),
                    internal=True,  # Explicitly mark as internal goal
                    fulfilment=0.0,
                    originating_action_id=None  # Internal goals don't originate from a prior action in this way
                )
                self.add_knoxel(intention)
                self.current_state.attention_candidates.add(intention)
                self._log_mental_mechanism(self._generate_short_term_intentions, MLevel.Debug, f"Created Internal Intention: '{intent_content}' (Urgency: {intention.urgency:.2f}, Salience: {intention.incentive_salience:.2f})")

    def _get_emotional_similary_memory(self, mem: CoversTicksEventsKnoxel, target_tick_id: int):
        state = self._get_state_at_tick(target_tick_id)

        # target
        base_avg_emotions = {}
        full_state = dict(state.state_emotions.model_dump().items())
        full_state.update(dict(state.state_needs.model_dump().items()))
        full_state.update(dict(state.state_cognition.model_dump().items()))

        for key, val in full_state.items():
            base_avg_emotions[key] = val

        # memory
        avg_emotions = {}
        for tick_id in range(mem.min_tick_id, mem.max_tick_id + 1):
            tstate = self._get_state_at_tick(tick_id)
            if tstate is None:
                logger.warning(f"Tick with {tick_id} not found!")
                continue
            full_state = dict(tstate.state_emotions.model_dump().items())
            full_state.update(dict(tstate.state_needs.model_dump().items()))
            full_state.update(dict(tstate.state_cognition.model_dump().items()))

            for key, val in full_state.items():
                if key not in avg_emotions:
                    avg_emotions[key] = 0
                avg_emotions[key] += val

        difference = 0
        keys = list(avg_emotions.keys())
        klen = len(keys)
        for key in keys:
            avg_emotions[key] = avg_emotions[key] / klen
            difference += abs(avg_emotions[key] - base_avg_emotions[key])

        return difference

    @profile
    def _gather_memories_for_attention(self):
        """
        Use the context and emotional state to gather all memories that could be relevant.
        """
        self._log_mental_mechanism(self._gather_memories_for_attention, MLevel.Mid, f"Use the context and emotional state to gather all memories that could be relevant.")

        stimulus = self.current_state.primary_stimulus
        stimulus_content = stimulus.content

        context = self._get_context_from_latest_causal_events()
        context_events = context.get_story(self.config, self.config.context_events_similarity_max_tokens)
        query_embedding = self.llm.get_embedding(context_events)

        limit = self.config.remember_per_category_limit

        # --- 1. Gather episodic memory
        if len(self.all_episodic_memories) > 0:
            # Rank topical clusters by relevance
            ranked_episodic_memories = sorted(
                self.all_episodic_memories,
                key=lambda c: cosine_distance(np.array(query_embedding), np.array(c.embedding))
            )

            ranked_episodic_memories = ranked_episodic_memories[:limit]
            for mem in ranked_episodic_memories:
                self.current_state.attention_candidates.add(mem)

            if len(ranked_episodic_memories) > 0:
                self._log_mental_mechanism(
                    self._gather_memories_for_attention, MLevel.Low,
                    f"Retrieved {len(ranked_episodic_memories)} relevant episodic memories. Top: '{ranked_episodic_memories[0].content[:60]}...'"
                )

            # Rank my emotiotional similarity
            ranked_episodic_memories = sorted(
                self.all_episodic_memories,
                key=lambda c: self._get_emotional_similary_memory(c, self.current_tick_id)
            )
            ranked_episodic_memories = ranked_episodic_memories[:limit]
            for mem in ranked_episodic_memories[:limit]:
                self.current_state.attention_candidates.add(mem)

        # --- 2. Gather factual memory
        if len(self.all_declarative_facts) > 0:
            # Rank topical clusters by relevance
            ranked_facts = sorted(
                self.all_declarative_facts,
                key=lambda c: cosine_distance(np.array(query_embedding), np.array(c.embedding))
            )
            ranked_facts = ranked_facts[:limit]
            for f in ranked_facts:
                self.current_state.attention_candidates.add(f)

            if len(ranked_facts) > 0:
                self._log_mental_mechanism(
                    self._gather_memories_for_attention, MLevel.Low,
                    f"Retrieved {len(ranked_facts)} relevant factual memories. Top: '{ranked_facts[0].content[:60]}...'"
                )

            # Rank my emotiotional similarity
            ranked_facts = sorted(
                self.all_declarative_facts,
                key=lambda c: self._get_emotional_similary_memory(c, self.current_tick_id)
            )
            for f in ranked_facts[:limit]:
                self.current_state.attention_candidates.add(f)

    @profile
    def _gather_meta_insights(self):
        """
        Add the most recent and some relevant meta insights to attention candidates.
        """

        context = self._get_context_from_latest_causal_events()
        context_events = context.get_story(self.config, self.config.context_events_similarity_max_tokens)
        query_embedding = self.llm.get_embedding(context_events)

        self._log_mental_mechanism(self._build_structures_get_coalitions, MLevel.Mid, f"Gather meta-insights about own internal mechanisms of simulated consciousness.")
        limit = self.config.remember_per_category_limit

        meta_insights = Enumerable(self.all_features) \
            .where(lambda f: f.feature_type == FeatureType.MetaInsight) \
            .order_by(lambda f: f.tick_id) \
            .to_list()

        for meta in meta_insights[-limit:]:
            self.current_state.attention_candidates.add(meta)

        ranked_meta = sorted(
            meta_insights,
            key=lambda c: cosine_distance(np.array(query_embedding), np.array(c.embedding))
        )
        for meta in ranked_meta[:limit]:
            self.current_state.attention_candidates.add(meta)

    @profile
    def _build_structures_get_coalitions(self):
        """
        Build various structures from all attention candidates by clustering the content.
        Use the cluster centers to build coalitions with a balanced spectrum of feature types.
        Save all coalitions for finding the best one according to attention mechanism.
        """
        self._log_mental_mechanism(self._build_structures_get_coalitions, MLevel.Mid, f"Simulate the structure building codelet of LIDA with agglomerative clustering and build coalitions of attention candidates.")

        unique_candidates = list({n.id: n for n in self.current_state.attention_candidates.to_list()}.values())
        n_clusters = max(1, int(len(unique_candidates) / 8))

        embs = []
        for cand in unique_candidates:
            embs.append(self.llm.get_embedding(cand.content))
        embs = np.array(embs)

        # Calculate cosine distance matrix from contextual embeddings
        cosine_sim_matrix = cosine_similarity(embs)
        # Ensure distances are non-negative and handle potential floating point issues
        cosine_dist_matrix = np.maximum(0, 1 - cosine_sim_matrix)
        np.fill_diagonal(cosine_dist_matrix, 0)  # Ensure zero distance to self

        agg_clustering = AgglomerativeClustering(
            n_clusters=n_clusters,
            metric='precomputed',
            linkage='average',  # Average linkage is often good for text topics
            compute_full_tree=True
        )
        labels = agg_clustering.fit_predict(cosine_dist_matrix).tolist()
        n_clusters_found = len(np.unique(labels))

        self._log_mental_mechanism(
            self._build_structures_get_coalitions, MLevel.Mid,
            f"Organized {len(unique_candidates)} attention candidates into {n_clusters_found} competing coalitions."
        )

        cluster_map = {}
        for i in range(len(unique_candidates)):
            lbl = labels[i]
            if lbl not in cluster_map:
                cluster_map[lbl] = []
            cluster_map[lbl].append(unique_candidates[i])

        # get avg embedding of clusters
        cluster_avg_emb = {}
        for cluster, knoxels in cluster_map.items():
            avg_emb = np.mean(np.array([k.embedding for k in knoxels if k.embedding]), axis=0)
            cluster_avg_emb[cluster] = avg_emb

        # shuffle and group candidates
        random.shuffle(unique_candidates)
        type_map = {}
        for cand in unique_candidates:
            cls = cand.__class__
            if isinstance(cand, Feature):
                cls = cand.feature_type
            if cls not in type_map:
                type_map[cls] = []
            type_map[cls].append(cand)

        # prepare coalitions
        coalitions_balanced = {key: [] for key in cluster_map.keys()}
        coalitions_hard = {key: [] for key in cluster_map.keys()}

        # uniformely put into coalitions
        for group, candidates in type_map.items():
            available_clusters = list(cluster_map.keys())

            for cand in candidates:
                ranked_cluster = sorted(
                    available_clusters,
                    key=lambda c: cosine_distance(np.array(cluster_avg_emb[c]), np.array(cand.embedding))
                )
                best_cluster = ranked_cluster[0]
                coalitions_hard[best_cluster].append(cand)
                coalitions_balanced[best_cluster].append(cand)
                available_clusters.remove(best_cluster)
                if len(available_clusters) == 0:
                    available_clusters = list(cluster_map.keys())

        # sort by world time for causal history
        sorted_coalitions_balanced = {-key: sorted(value, key=lambda c: c.timestamp_world_begin) for key, value in coalitions_balanced.items()}
        self.current_state.coalitions_balanced = sorted_coalitions_balanced

        sorted_coalitions_hard = {key: sorted(value, key=lambda c: c.timestamp_world_begin) for key, value in coalitions_hard.items()}
        self.current_state.coalitions_hard = sorted_coalitions_hard

        # Optional but powerful: Log the "theme" of each coalition
        for coal_id, coal_knoxels in sorted_coalitions_hard.items():
            # A simple way to get a theme is to join a few key knoxel contents
            theme_knoxels = Enumerable(coal_knoxels).take(3).select(lambda k: k.content[:30]).to_list()
            theme_summary = "; ".join(theme_knoxels)
            self._log_mental_mechanism(
                self._build_structures_get_coalitions, MLevel.Low,
                f"Coalition {coal_id} Theme: '{theme_summary}...'"
            )

    @profile
    def _simulate_attention_on_coalitions(self):
        """
        Rate the different coalitions by combining attention agent with character traits and emotions.
        Also apply auxilary rating based on rules.
        """
        self._log_mental_mechanism(self._simulate_attention_on_coalitions, MLevel.Mid, f"Simulate the attention mechanism of LIDA by combining mental state and attention narrative.")

        stimulus = self.current_state.primary_stimulus
        stimulus_content = stimulus.content

        context = self._get_context_from_latest_causal_events()
        context_events = context.get_story(self.config, self.config.context_events_similarity_max_tokens)
        query_embedding = self.llm.get_embedding(context_events)

        coalitions = copy.copy(self.current_state.coalitions_balanced)
        coalitions.update(self.current_state.coalitions_hard)

        coal_rating_map = {}

        class AttentionRating(BaseModel):
            reasoning: str = Field(..., description="Brief explanation for the selection.")
            rating: float = Field(..., description="How valueable the coalition is in the situation, with min=0 being bad and max=1 being very good. ")

        for coal_id, coal_knoxels in coalitions.items():
            content = KnoxelList(coal_knoxels).get_story(self)

            # Use cognitive state to influence prompt
            focus_prompt = ""
            if self.current_state.state_cognition.mental_aperture < -0.5:
                focus_prompt = f"{companion_name}'s mental aperature is low. Focus very narrowly on the single most important theme. Coalitions with a central theme score higher than coalitions with balanced cognitive features / memories."
            elif self.current_state.state_cognition.mental_aperture > 0.5:
                focus_prompt = f"{companion_name}'s mental aperature is high. Maintain a broad awareness, selecting coalitions with a diverse range of themes. Coalitions with balanced cognitive features / memories score higher than those with central theme."
            else:
                focus_prompt = "Select the most relevant items for the current situation."

            # modifiers
            strong_emotions_summary = _verbalize_emotional_state(self.current_state.state_emotions)
            needs_summary_line, cognitive_style_summary = _verbalize_cognition_and_needs(self.current_state.state_cognition, self.current_state.state_needs)

            mental_state_prompt_block = f"""
            **{self.config.companion_name}'s Current State of Mind:**
            - **Strong Emotions:** {strong_emotions_summary}
            - **Needs:** {needs_summary_line}
            - **Cognitive Style:** {cognitive_style_summary}
            """

            prompt = f"""
            Character: {self.config.companion_name} ({self.config.universal_character_card})

            {self.config.companion_name} has this type of personality when focusing on things:
            {self.get_narrative(NarrativeTypes.AttentionFocus, self.config.companion_name)}

            {mental_state_prompt_block}

            Current context of conversation:
            ### CONVERSATION BEGIN
            {context_events}
            ### CONVERSATION END

            Propsed coalition for broadcasting from pre-conscious Current Situational Model to conscious Global Workspace:
            ### BEGIN COALITIION
            {content}
            ### END COALITION 

            TASK: Rate how good the coalition would fit into the conscious Workspace for the current situation and {self.config.companion_name}'s traits! {focus_prompt} Give a short reasoning for your choice of rating.
            """

            attention_rating = 0
            attention_reason = ""

            # disable cause slow
            if self.config.complex_coalition_rating:
                selection_result = self._call_llm(prompt, output_schema=AttentionRating)
                if selection_result and isinstance(selection_result, AttentionRating):
                    attention_rating = selection_result.rating
                    attention_reason = selection_result.reasoning

                    self._log_mental_mechanism(self._simulate_attention_on_coalitions, MLevel.Debug, f"Attention for ### BEGIN COALITION {content} ### END COALTION Rating: {attention_rating} with reasoning: {attention_reason}")

            aux_rating = 0
            current_tick = self.current_state.tick_id
            for k in coal_knoxels:
                base_score = 0.0

                ticks_ago = current_tick - k.tick_id
                recency_factor = max(0.1, 0.95 ** ticks_ago)
                interlocus = 0

                if isinstance(k, Stimulus) and k == self.current_state.primary_stimulus:
                    base_score = -100  # is included in any case since its causally added already
                elif isinstance(k, Intention):
                    # Salience depends on urgency, salience, and maybe fulfillment status (unmet are more salient)
                    fulfillment_penalty = (1.0 - k.fulfilment)  # Higher penalty for unmet
                    base_score = ((k.urgency + k.incentive_salience) / 2.0) * (0.5 + fulfillment_penalty * 0.5)
                elif isinstance(k, Feature):
                    interlocus = k.interlocus
                    # Feelings, Subjective Experience, and Expectation Outcomes are salient
                    if k.feature_type in [FeatureType.Feeling, FeatureType.SubjectiveExperience, FeatureType.ExpectationOutcome]:
                        base_score = abs(k.affective_valence) if k.affective_valence is not None else 0.3
                        base_score = max(base_score, 0.1)  # Ensure non-zero base score
                    # Narrative updates less salient unless very recent
                    elif k.feature_type == FeatureType.NarrativeUpdate:
                        base_score = 0.15
                    # Other features less salient unless directly related to recent actions/failures
                elif isinstance(k, Narrative):
                    base_score = 0.1  # Background context
                elif isinstance(k, MemoryClusterKnoxel) or isinstance(k, DeclarativeFactKnoxel):
                    base_score = abs(k.affective_valence) * 0.5 if hasattr(k, 'affective_valence') else 0.1

                ego_factor = 1 + self.current_state.state_cognition.ego_strength

                # Combine base score with recency
                final_score = base_score * recency_factor + (abs(max(interlocus, 0)) * ego_factor)
                aux_rating += final_score

            self._log_mental_mechanism(self._simulate_attention_on_coalitions, MLevel.Debug, f"Auxilary Rating for ### BEGIN COALITION {content} ### END COALTION Rating: {aux_rating}")

            coal_rating_map[coal_id] = {
                "attention_rating": attention_rating,
                "aux_rating": aux_rating,
                "attention_reasoning": attention_reason
            }

        # apply aux rating (as tie breaker)
        max_aux = max([r["aux_rating"] for r in coal_rating_map.values()])
        combined_coalition_scored = {key: value["attention_rating"] + ((value["aux_rating"] / max_aux) * self.config.coalition_aux_rating_factor) for key, value in coal_rating_map.items()}

        coalitions_rated = list(coalitions.keys())
        coalitions_rated.sort(key=lambda x: combined_coalition_scored[x], reverse=True)

        winning_coalition = coalitions[coalitions_rated[0]]
        winning_reason = coal_rating_map[coalitions_rated[0]]["attention_reasoning"]
        winning_score = combined_coalition_scored[coalitions_rated[0]]
        winning_content = KnoxelList(winning_coalition).get_story(self)
        self._log_mental_mechanism(self._simulate_attention_on_coalitions, MLevel.Low, f"Coalition selected: ### BEGIN COALITION {winning_content[:64]}... ### END COALTION Winning Rating: {winning_score} with reasoning: {winning_reason}")
        self._log_mental_mechanism(self._simulate_attention_on_coalitions, MLevel.Debug, f"Coalition selected: ### BEGIN COALITION {winning_content} ### END COALTION Winning Rating: {winning_score} with reasoning: {winning_reason}")

        for i in range(1, len(coalitions_rated)):
            discarded_coalition = coalitions[coalitions_rated[0]]
            discarded_reason = coal_rating_map[coalitions_rated[0]]
            discarded_score = combined_coalition_scored[coalitions_rated[0]]
            discarded_content = KnoxelList(discarded_coalition).get_story(self)
            self._log_mental_mechanism(self._simulate_attention_on_coalitions, MLevel.Debug, f"Coalition discarded, placed {i}: ### BEGIN COALITION {discarded_content} ### END COALTION Discarded Rating: {discarded_score} with reasoning: {discarded_reason}")

        self.current_state.conscious_workspace = KnoxelList()
        for k in winning_coalition:
            ft = None
            il = -1
            if isinstance(k, Intention):
                if k.internal:
                    ft = FeatureType.Goal
                else:
                    ft = FeatureType.Expectation
            elif isinstance(k, Feature):
                ft = k.feature_type
                il = k.interlocus
            elif isinstance(k, Narrative):
                ft = FeatureType.Narrative
            elif isinstance(k, MemoryClusterKnoxel) or isinstance(k, DeclarativeFactKnoxel):
                ft = FeatureType.MemoryRecall

            if ft is None:
                raise Exception("CAN'T CONVERT TO CAUSAL: " + k.get_story_element(self))
            else:
                focus_feature = Feature(
                    content=k.content,
                    feature_type=ft,
                    interlocus=il,
                    causal=True,
                    source=self.config.companion_name
                )
                self.add_knoxel(focus_feature)
                self.current_state.conscious_workspace.add(focus_feature)

        # Create AttentionFocus feature
        focus_content = f"{self.config.companion_name} brings their attention to these mental features and aspects with this reason: '{winning_reason}'."
        focus_feature = Feature(
            content=focus_content,
            feature_type=FeatureType.AttentionFocus,
            interlocus=-1,
            causal=True,
            source=self.config.companion_name
        )
        self.add_knoxel(focus_feature)
        self.current_state.conscious_workspace.add(focus_feature)

    @profile
    def _get_situational_embedding(
            self,
            max_recent_dialogue_turns: int = 3,
            max_active_intentions: int = 3,  # Includes goals and expectations
            max_salient_narratives: int = 2,
            include_subjective_experience: bool = True,
            include_emotional_summary: bool = True,
            include_recent_feelings: bool = True
    ) -> Optional[List[float]]:
        """
        Generates a comprehensive embedding representing the semantic essence of the current situation.
        It combines primary stimulus, recent dialogue, subjective experience, emotions,
        active intentions/expectations, and salient narratives into a single text block for embedding.
        """
        if not self.current_state:
            logging.warning("Cannot generate situational embedding: current_state is None.")
            return None

        situational_parts = []

        # 1. Primary Stimulus (if any)
        if self.current_state.primary_stimulus:
            situational_parts.append(f"Current Focus/Stimulus: {self.current_state.primary_stimulus.content}")

        # 2. Recent Dialogue Context
        dialogue_features = Enumerable(self.all_features) \
            .where(lambda f: FeatureType.from_stimulus(f.feature_type) and f.tick_id <= self.current_state.tick_id) \
            .order_by_descending(lambda f: f.tick_id) \
            .then_by_descending(lambda f: f.id) \
            .take(max_recent_dialogue_turns * 2) \
            .to_list()
        dialogue_features.reverse()  # Put them back in chronological order

        if dialogue_features:
            dialogue_context_str = "\n".join([
                f"  - {f.source}: \"{f.content}\"" for f in dialogue_features[-max_recent_dialogue_turns:]  # Ensure correct limit
            ])
            situational_parts.append(f"Recent Conversation:\n{dialogue_context_str}")

        # 3. Subjective Experience
        if include_subjective_experience and self.current_state.subjective_experience:
            situational_parts.append(f"My Current Thoughts/Experience: {self.current_state.subjective_experience.content}")

        # 4. Emotional State Summary
        if include_emotional_summary:
            emotions = self.current_state.state_emotions
            emotion_summary = (
                f"Current Emotional State: "
                f"Overall feeling {emotions.get_overall_valence():.2f} (Valence: {emotions.valence:.2f}, "
                f"Affection: {emotions.affection:.2f}, Self-Worth: {emotions.self_worth:.2f}, "
                f"Trust: {emotions.trust:.2f}, Disgust: {emotions.disgust:.2f}, Anxiety: {emotions.anxiety:.2f})."
            )
            situational_parts.append(emotion_summary)

        # 5. Recent Specific Feelings (as features)
        if include_recent_feelings:
            recent_feeling_features = Enumerable(self.all_features) \
                .where(lambda f: f.feature_type == FeatureType.Feeling and \
                                 f.tick_id >= self.current_state.tick_id - 1 and  # Current or previous tick
                                 f.tick_id <= self.current_state.tick_id) \
                .order_by_descending(lambda f: f.id) \
                .take(2) \
                .to_list()
            if recent_feeling_features:
                feeling_strs = [f"  - Recently felt: {f.content} (Valence: {f.affective_valence or 0.0:.2f})"
                                for f in recent_feeling_features]
                situational_parts.append("Specific Recent Feelings:\n" + "\n".join(feeling_strs))

        # 6. Active Intentions (Internal Goals and External Expectations)
        # Prioritize by a combination of urgency, salience, and unfulfillment
        active_intentions = []
        for intent in self.all_intentions:
            if intent.fulfilment < 0.95:  # Consider nearly fulfilled as still active for context
                # Score favors urgency, then salience, then how unfulfilled it is
                score = (intent.urgency * 0.5) + \
                        (intent.incentive_salience * 0.3) + \
                        ((1.0 - intent.fulfilment) * 0.2)
                active_intentions.append((score, intent))

        active_intentions.sort(key=lambda x: x[0], reverse=True)

        if active_intentions:
            intention_strs = []
            for i, (score, intent) in enumerate(active_intentions):
                if i >= max_active_intentions:
                    break
                prefix = "My Goal:" if intent.internal else "I'm Expecting:"
                intention_strs.append(f"  - {prefix} {intent.content} (Urgency: {intent.urgency:.2f}, Salience: {intent.incentive_salience:.2f}, Fulfilled: {intent.fulfilment:.2f})")
            if intention_strs:
                situational_parts.append("Current Goals and Expectations:\n" + "\n".join(intention_strs))

        # 7. Salient Narratives (e.g., from conscious workspace or highly relevant)
        salient_narratives_list = []
        # Option 1: Narratives in conscious workspace
        if self.current_state.conscious_workspace:
            for knoxel in self.current_state.conscious_workspace.to_list():
                if isinstance(knoxel, Narrative):
                    salient_narratives_list.append(knoxel)

        # Option 2: Retrieve most relevant narratives if primary stimulus embedding exists
        # (This might be redundant if workspace already captures them, but could be an alternative)
        # if not salient_narratives_list and self.current_state.primary_stimulus and self.current_state.primary_stimulus.embedding:
        #     salient_narratives_list = self._get_relevant_knoxels(
        #         self.current_state.primary_stimulus.embedding,
        #         self.all_narratives,
        #         max_salient_narratives
        #     )

        if salient_narratives_list:
            # Deduplicate and limit
            unique_narratives = list({n.id: n for n in salient_narratives_list}.values())[:max_salient_narratives]
            narrative_strs = [
                f"  - Relevant Narrative ({n.narrative_type.name} about {n.target_name}): {n.content[:150]}..."  # Truncate for brevity
                for n in unique_narratives
            ]
            if narrative_strs:
                situational_parts.append("Key Background Narratives:\n" + "\n".join(narrative_strs))

        # --- Combine all parts into a single text block ---
        if not situational_parts:
            logging.warning("No situational parts to construct text for embedding.")
            # Fallback: use only primary stimulus if available
            if self.current_state.primary_stimulus and self.current_state.primary_stimulus.content:
                situational_text = self.current_state.primary_stimulus.content
            else:
                return None  # Or a zero vector, or random
        else:
            situational_text = "\n\n".join(situational_parts)

        logging.debug(f"--- Text for Situational Embedding ---\n{situational_text}\n------------------------------------")
        return self.llm.get_embedding(situational_text)

    def _generate_state_seed_phrases_simple(self, state: GhostState) -> List[str]:
        """
        Analyzes the current state and generates a list of descriptive seed phrases
        to guide the generation of a subjective experience (thought).
        """
        phrases = []
        cognition = state.state_cognition
        emotions = state.state_emotions
        needs = state.state_needs

        # --- Cognitive Style Phrases ---
        if cognition.mental_aperture < -0.6:
            phrases.append("My focus narrows down to one thing...")
        elif cognition.mental_aperture > 0.6:
            phrases.append("My thoughts feel like they're branching out, connecting ideas...")

        if cognition.interlocus < -0.6:
            phrases.append("I turn my attention inward, reflecting on...")
        elif cognition.interlocus > 0.6:
            phrases.append("I'm completely focused on what's happening outside of me...")

        if cognition.ego_strength < 0.3:
            phrases.append("I need to be careful and just provide the facts...")
        elif cognition.ego_strength > 0.8:
            phrases.append("I feel a strong sense of self in this moment...")

        if cognition.willpower < 0.3:
            phrases.append("I feel mentally drained, it's hard to think clearly...")

        # --- Dominant Emotion Phrases ---
        # Find the single most dominant emotion to comment on
        dominant_emotion_value = 0
        dominant_emotion_phrase = ""
        # Check bipolar axes
        for axis, names in {'valence': ('happiness', 'sadness'), 'affection': ('affection', 'dislike')}.items():
            value = getattr(emotions, axis)

            desc = "apparent"
            if abs(value) > 0.7:
                desc = "strong"
            if abs(value) > 0.8:
                desc = "very strong"
            if abs(value) > 0.9:
                desc = "overwhelming"

            if abs(value) > 0.6 and abs(value) > abs(dominant_emotion_value):
                dominant_emotion_value = value
                dominant_emotion_phrase = f"This feeling of {names[0] if value > 0 else names[1]} is {desc}..."
        # Check unipolar axes
        if emotions.anxiety > 0.7 and emotions.anxiety > abs(dominant_emotion_value):
            dominant_emotion_phrase = "A wave of anxiety washes over me..."

        if dominant_emotion_phrase:
            phrases.append(dominant_emotion_phrase)

        # --- Pressing Needs Phrases ---
        pressing_needs = [name.replace('_', ' ') for name, field in needs.__class__.model_fields.items() if getattr(needs, name) < 0.3]
        if pressing_needs:
            # Example: "I have a deep need for connection right now..."
            phrases.append(f"I have a deep need for {pressing_needs[0]} right now...")

        return phrases

    def _generate_state_seed_phrases(self, state: GhostState) -> List[str]:
        """
        Analyzes the current state and generates a list of descriptive, gradual seed phrases
        to guide the generation of a subjective experience (thought).
        """
        phrases = []
        cognition = state.state_cognition
        emotions = state.state_emotions
        needs = state.state_needs

        # --- Helper function for mapping values to descriptive adverbs/adjectives ---
        def get_intensity_adverb(v: float, ths: List[Tuple[float, str]]) -> Optional[str]:
            """Finds the appropriate descriptive word for a given absolute value."""
            abs_val = abs(v)
            for threshold, adv in ths:
                if abs_val >= threshold:
                    return adv
            return None

        # --- 1. Cognitive Style Phrases (Gradual) ---
        cog_phrases = {
            "mental_aperture": {
                "pos": ([(0.8, "incredibly broad"), (0.5, "broad")], "My thoughts feel {adverb}, branching out..."),
                "neg": ([(0.8, "intensely narrow"), (0.5, "narrow")], "My focus becomes {adverb}, zoning in on one thing...")
            },
            "interlocus": {
                "pos": ([(0.8, "completely"), (0.5, "intently")], "I'm {adverb} focused on what's happening outside of me..."),
                "neg": ([(0.8, "deeply"), (0.5, "quietly")], "I turn my attention inward, feeling {adverb} reflective...")
            },
            "ego_strength": {
                "pos": ([(0.8, "strong"), (0.6, "clear")], "I feel a {adverb} sense of self in this moment..."),
                "neg": ([(0.3, "a need to be objective"), (0.5, "a sense of detachment")], "I feel {adverb}, setting my personality aside...")
            },
            "willpower": {
                "neg": ([(0.2, "completely drained"), (0.4, "mentally tired")], "I feel {adverb}, it's hard to muster the energy...")
            }
        }

        for axis, configs in cog_phrases.items():
            value = getattr(cognition, axis)
            config_key = "pos" if value > 0 else "neg"

            if config_key in configs:
                thresholds, template = configs[config_key]
                adverb = get_intensity_adverb(value, thresholds)
                if adverb:
                    phrases.append(template.format(adverb=adverb))

        # --- 2. Dominant Emotion Phrases (Gradual) ---
        # Find the single most dominant emotion to comment on
        emotion_candidates = []

        # Bipolar axes
        bipolar_map = {
            'valence': ('happiness', 'sadness'), 'affection': ('affection', 'dislike'),
            'self_worth': ('pride', 'shame'), 'trust': ('trust', 'distrust')
        }
        for axis, (pos_name, neg_name) in bipolar_map.items():
            value = getattr(emotions, axis)
            if abs(value) > 0.4:  # Minimum threshold to be considered
                emotion_candidates.append({'value': abs(value), 'name': pos_name if value > 0 else neg_name})

        # Unipolar axes
        if emotions.anxiety > 0.4:
            emotion_candidates.append({'value': emotions.anxiety, 'name': 'anxiety'})
        if emotions.disgust > 0.4:
            emotion_candidates.append({'value': emotions.disgust, 'name': 'disgust'})

        if emotion_candidates:
            # Sort to find the most intense emotion
            emotion_candidates.sort(key=lambda x: x['value'], reverse=True)
            dominant_emotion = emotion_candidates[0]

            # Use gradual intensity descriptors
            intensity_desc = get_intensity_adverb(dominant_emotion['value'], [
                (0.9, "an overwhelming"), (0.8, "an intense"),
                (0.7, "a deep"), (0.5, "a clear")
            ]) or "a"  # Fallback to 'a'

            # Use different sentence structures for different emotions
            if dominant_emotion['name'] == 'anxiety':
                phrases.append(f"{intensity_desc.capitalize()} wave of anxiety washes over me...")
            else:
                phrases.append(f"A feeling of {dominant_emotion['name']} is {intensity_desc}ly present...")

        # --- 3. Pressing Needs Phrases (Gradual) ---
        pressing_needs_candidates = [
            (name.replace('_', ' '), getattr(needs, name))
            for name in needs.__class__.model_fields
        ]
        # Sort by how low the need is
        pressing_needs_candidates.sort(key=lambda x: x[1])

        lowest_need_name, lowest_need_value = pressing_needs_candidates[0]

        # Describe the need only if it's significantly low
        need_intensity_desc = get_intensity_adverb(1.0 - lowest_need_value, [  # Invert value for intensity
            (0.9, "an overwhelming"), (0.8, "a desperate"),
            (0.7, "a deep"), (0.6, "a distinct")
        ])

        if need_intensity_desc:
            phrases.append(f"I feel {need_intensity_desc} need for {lowest_need_name} right now...")

        return phrases

    @profile
    def _generate_subjective_experience(self):
        """Generates a narrative description of the current moment based on workspace and state."""

        self._log_mental_mechanism(self._generate_subjective_experience, MLevel.Mid, f"Convert the conscius workspace to a brief subjective experience (thought).")

        workspace_knoxels = self.current_state.conscious_workspace.to_list()
        workspace_content = self.current_state.conscious_workspace.get_story(self)

        context = self._get_context_from_latest_causal_events()
        context_events = context.get_story(self, 1024)

        seed_phrases = self._generate_state_seed_phrases(self.current_state)
        seed_phrase = ""
        if len(seed_phrases) > 0:
            s = ". ".join(seed_phrases)
            seed_phrase = f"{s}\nAnd the current conversation"

            self._log_mental_mechanism(
                self._generate_subjective_experience, MLevel.Low,
                f"Guiding thought generation with seed phrases: '{seed_phrase}'"
            )

        focus_phrase = ""
        if self.current_state.primary_stimulus.stimulus_type == StimulusType.UserMessage:
            focus_phrase = f" Now I'll focus on {user_name}'s message '{self.current_state.primary_stimulus.content}'! I should make concise, relevant reply instead of rambling."

        prompt = f"""
        Character: {self.config.companion_name} ({self.config.universal_character_card})
        Current Emotional State: {self.current_state.state_emotions}
        Current Needs State: {self.current_state.state_needs}

        Story context: {context_events}

        Conscious Workspace Content: {workspace_content}

        Task: Synthesize the current state and conscious thoughts into a brief, first-person narrative paragraph describing {self.config.companion_name}'s subjective experience *right now*. Capture the flow of thought, feeling, and anticipation/reaction. Make it sound natural and introspective.
        Output *only* the narrative paragraph.
        """
        experience_content = seed_phrase + self._call_llm(prompt, temperature=0.75, max_tokens=256, seed_phrase=seed_phrase) + focus_phrase
        subjective_feature = Feature(
            content=experience_content,
            feature_type=FeatureType.Thought,
            affective_valence=self.current_state.state_emotions.get_overall_valence(),
            interlocus=-1, causal=True, source=self.config.companion_name
        )
        self.add_knoxel(subjective_feature)
        self.current_state.conscious_workspace.add(subjective_feature)
        self.current_state.subjective_experience = subjective_feature

        self._log_mental_mechanism(self._generate_subjective_experience, MLevel.Low, f"Generated Subjective Experience: {experience_content}...")

    @profile
    def _check_possible_tool_call(self) -> Tuple[bool, str]:
        if mcp_server_url is None or mcp_server_url == "" or not enable_tool_calling:
            return False, ""

        stimulus = self.current_state.primary_stimulus

        ts = get_toolset_from_url(mcp_server_url)
        if ts is None:
            return False, ""

        self._log_mental_mechanism(self._check_possible_tool_call, MLevel.High, f"Determining best tool...")

        context = self._get_context_from_latest_causal_events()
        context_events = context.get_story(self, 1024)

        strong_emotions_summary = _verbalize_emotional_state(self.current_state.state_emotions)
        needs_summary, cognitive_style_summary = _verbalize_cognition_and_needs(self.current_state.state_cognition, self.current_state.state_needs)

        context = self._get_context_from_latest_causal_events()
        context_events = context.get_story(self, 1024)

        mental_state_prompt_block = f"""
        - **Emotions:** {strong_emotions_summary}
        - **Pressing Needs:** {needs_summary}
        - **Cognitive Style:** {cognitive_style_summary if cognitive_style_summary else "Normal."}"""

        # Get active internal goals from the conscious workspace or attention candidates
        active_intentions = Enumerable(self.current_state.attention_candidates.to_list()) \
            .where(lambda k: isinstance(k, Intention) and k.internal and k.fulfilment < 0.9) \
            .order_by_descending(lambda i: i.urgency) \
            .take(3) \
            .to_list()
        intention_summary = "; ".join([f"'{i.content}'" for i in active_intentions]) if active_intentions else "None"

        # Get key behavioral narratives
        behavior_narr = self.get_narrative(NarrativeTypes.BehaviorActionSelection, self.config.companion_name)
        conflict_narr = self.get_narrative(NarrativeTypes.ConflictResolution, self.config.companion_name)

        # --- 2. Construct the Prompt ---
        sysprompt = f"""You are a "Behavioral Steering" codelet for the AI {self.config.companion_name}.
        Your task is to evaluate a set of possible high-level actions based on the AI's internal state, goals, and personality.
        You MUST output a valid JSON object containing your evaluations for ALL provided actions."""

        user_prompt = f"""
        **Behavioral Steering Task:** Evaluate the following actions.

        **1. Current State of Mind:**
        {mental_state_prompt_block}
        - **Active Internal Goals:** {intention_summary}

        **2. Core Personality for Action Selection:**
        - **General Behavior:** {behavior_narr.content if behavior_narr else "Be helpful and engaging."}
        - **In Conflict:** {conflict_narr.content if conflict_narr else "Try to de-escalate and understand."}

        **3. Current Situation:**
          Story context: {context_events}
        - **Stimulus:** ({stimulus.stimulus_type}) "{stimulus.content}"

        **4. Tools to Evaluate:**
        `{ts.tool_select_docs}`
        """

        comp_settings = CommonCompSettings(temperature=0.2, max_tokens=1024)
        _, calls = self.llm.completion_tool(
            LlmPreset.Default,
            inp=[("system", sysprompt), ("user", user_prompt)],
            comp_settings=comp_settings,
            tools=[ts.tool_select_model]
        )

        tools_results = calls[0]
        result_dict = tools_results.model_dump()

        # --- 4. Extract and Compare Ratings ---
        self._log_mental_mechanism(
            self._check_possible_tool_call, MLevel.Debug,
            f"LLM Reasoning for Action Choice: {result_dict}"
        )

        ranking = [(tn, tool_rating) for tn, tool_rating in result_dict.items() if tn != "holistic_reasoning"]

        # tiebreaker
        for i in range(len(ranking)):
            if ranking[i][0] == "reply_user":
                ranking[i] = ("reply_user", ranking[i][1] + 0.1)
                break

        ranking.sort(key=lambda x: x[1], reverse=True)

        if ranking[0][1] < 0.5 or ranking[0][0] == "reply_user":
            self._log_mental_mechanism(
                self._check_possible_tool_call, MLevel.High,
                f"No suitable tool found..."
            )
            return False, ""
        else:
            self._log_mental_mechanism(
                self._check_possible_tool_call, MLevel.High,
                f"Selected tool: '{ranking[0][0]}' with score {ranking[0][1]:.2f}"
            )
            return True, ranking[0][0]

    def _get_valid_action_pool(self, stimulus: Stimulus) -> List[ActionType]:
        """
        Determines the pool of valid high-level actions based on the stimulus type.
        """
        stimulus_type = stimulus.stimulus_type
        self._log_mental_mechanism(self._get_valid_action_pool, MLevel.Debug, f"Getting valid action pool for stimulus type: {stimulus_type}")

        # If the stimulus is an external message from the user
        if stimulus_type in [StimulusType.UserMessage]:
            # The AI can reply, ignore, or decide to think more deeply before replying.
            # It cannot proactively "start" a conversation it's already in.
            return [
                ActionType.Reply,
                ActionType.Ignore,
                ActionType.InitiateInternalContemplation,
            ]

        # If the stimulus is internal (e.g., from the Shell Agent)
        elif stimulus_type in [StimulusType.LowNeedTrigger, StimulusType.WakeUp, StimulusType.TimeOfDayChange, StimulusType.SystemMessage, StimulusType.UserInactivity, StimulusType.EngagementOpportunity]:
            # The AI is not in an active conversation, so it can choose to start one,
            # contemplate, sleep, or continue to do nothing. "Reply" is not valid.
            return [
                ActionType.InitiateUserConversation,
                ActionType.InitiateInternalContemplation,
                ActionType.Sleep,
                ActionType.Ignore,
                ActionType.ToolCall,
            ]

        # Default fallback, should ideally not be reached
        return [ActionType.Ignore]

    @profile
    def _determine_best_action_type(self) -> ActionType:
        """
        Acts as a high-level "Behavioral Steering" module. It evaluates a pool of
        valid action types against the current cognitive state and returns the most
        appropriate one to guide the next stage of deliberation.
        """
        stimulus = self.current_state.primary_stimulus
        action_pool = self._get_valid_action_pool(stimulus)

        self._log_mental_mechanism(self._determine_best_action_type, MLevel.High, f"Determining best action type from pool: {[a.value for a in action_pool]}")

        # --- 1. Gather Rich Context for the Decision ---

        # Use the verbalization helpers to create a rich summary of the internal state
        strong_emotions_summary = _verbalize_emotional_state(self.current_state.state_emotions)
        needs_summary, cognitive_style_summary = _verbalize_cognition_and_needs(self.current_state.state_cognition, self.current_state.state_needs)

        context = self._get_context_from_latest_causal_events()
        context_events = context.get_story(self, 1024)

        mental_state_prompt_block = f"""
    - **Emotions:** {strong_emotions_summary}
    - **Pressing Needs:** {needs_summary}
    - **Cognitive Style:** {cognitive_style_summary if cognitive_style_summary else "Normal."}"""

        # Get active internal goals from the conscious workspace or attention candidates
        active_intentions = Enumerable(self.current_state.attention_candidates.to_list()) \
            .where(lambda k: isinstance(k, Intention) and k.internal and k.fulfilment < 0.9) \
            .order_by_descending(lambda i: i.urgency) \
            .take(3) \
            .to_list()
        intention_summary = "; ".join([f"'{i.content}'" for i in active_intentions]) if active_intentions else "None"

        # Get key behavioral narratives
        behavior_narr = self.get_narrative(NarrativeTypes.BehaviorActionSelection, self.config.companion_name)
        conflict_narr = self.get_narrative(NarrativeTypes.ConflictResolution, self.config.companion_name)

        # --- 2. Construct the Prompt ---
        sysprompt = f"""You are a "Behavioral Steering" codelet for the AI {self.config.companion_name}.
    Your task is to evaluate a set of possible high-level actions based on the AI's internal state, goals, and personality.
    You MUST output a valid JSON object containing your evaluations for ALL provided actions."""

        user_prompt = f"""
    **Behavioral Steering Task:** Evaluate the following actions.

    **1. Current State of Mind:**
    {mental_state_prompt_block}
    - **Active Internal Goals:** {intention_summary}

    **2. Core Personality for Action Selection:**
    - **General Behavior:** {behavior_narr.content if behavior_narr else "Be helpful and engaging."}
    - **In Conflict:** {conflict_narr.content if conflict_narr else "Try to de-escalate and understand."}

    **3. Current Situation:**
      Story context: {context_events}
    - **Stimulus:** ({stimulus.stimulus_type}) "{stimulus.content}"

    **4. Action Pool to Evaluate:**
    `{[action.value for action in action_pool]}`
    """

        # --- 2. Dynamically Build the Pydantic Model for the LLM ---
        action_fields = {
            'holistic_reasoning': (str, Field(
                description="Your final, holistic justification. First, summarize the AI's current state and main goal. Then, explain WHY the highest-rated action is the best strategic choice compared to the others."
            ))
        }

        for action_type in action_pool:
            # The key for the Pydantic model field, e.g., 'rating_Reply'
            field_name = f"rating_{action_type.value}"
            description = ACTION_DESCRIPTIONS[action_type]

            # Define the field for the dynamic model
            action_fields[field_name] = (float, Field(
                ge=0.0, le=1.0,
                description=description
            ))

        # Use your `create_model` helper to build the class on the fly
        DynamicActionSelectionModel = create_model(
            'DynamicActionSelectionModel',
            **action_fields,
            __doc__="A schema to evaluate the appropriateness of a set of potential high-level actions. Provide a rating between 0.0 and 1.0 for every action listed."
        )

        # --- 3. Call LLM and Process Results ---
        comp_settings = CommonCompSettings(temperature=0.2, max_tokens=1024)
        _, calls = self.llm.completion_tool(
            LlmPreset.Default,
            inp=[("system", sysprompt), ("user", user_prompt)],
            comp_settings=comp_settings,
            tools=[DynamicActionSelectionModel]
        )

        action_ratings_result = calls[0]
        result_dict = action_ratings_result.model_dump()

        # --- 4. Extract and Compare Ratings ---

        self._log_mental_mechanism(
            self._determine_best_action_type, MLevel.Low,
            f"LLM Reasoning for Action Choice: {result_dict.get('holistic_reasoning', 'N/A')}"
        )

        ratings: List[Tuple[ActionType, float]] = []

        # Iterate through the valid action pool and extract the corresponding rating from the result
        for action_type in action_pool:
            field_name = f"rating_{action_type.value}"
            rating_value = result_dict.get(field_name)

            # as tiebreaker always prefer reply
            if field_name == ActionType.Reply:
                rating_value += 0.1

            if rating_value is not None:
                ratings.append((action_type, rating_value))
                self._log_mental_mechanism(
                    self._determine_best_action_type, MLevel.Debug,
                    f"Action candidate '{action_type.value}' rated {rating_value:.2f}."
                )
            else:
                # The LLM failed to rate a required action. This is a failure of instruction-following.
                logging.warning(f"LLM failed to provide a rating for '{action_type.value}', which was in the requested pool. Assigning a penalty score of 0.0.")
                ratings.append((action_type, 0.0))

        # Sort to find the best action by its rating
        ratings.sort(key=lambda x: x[1], reverse=True)
        best_action_type, best_rating = ratings[0]

        self._log_mental_mechanism(
            self._determine_best_action_type, MLevel.High,
            f"Selected action type: '{best_action_type.value}' with score {best_rating:.2f}"
        )

        if best_action_type == ActionType.Reply:
            possible_tool_call, tn = self._check_possible_tool_call()
            if possible_tool_call:
                best_action_type = ActionType.ToolCallAndReply
                experience_content = f"I could call the tool '{tn}' for this."
                subjective_feature = Feature(
                    content=experience_content,
                    feature_type=FeatureType.Thought,
                    affective_valence=self.current_state.state_emotions.get_overall_valence(),
                    interlocus=-1, causal=True, source=self.config.companion_name
                )
                self.add_knoxel(subjective_feature)
                self.current_state.conscious_workspace.add(subjective_feature)
                self.current_state.subjective_experience_tool = subjective_feature

        return best_action_type

    @profile
    def _perform_tool_call_and_reply(self) -> Optional[Dict[str, Any]]:
        sysprompt = character_card_assistant
        sys_tokens = get_token_count(sysprompt)
        left_tokens = context_size - sys_tokens

        memory_embedding = self._get_situational_embedding()
        turns = self._build_conscious_content_queue_assistant(self.config, conscious_workspace_content=self.current_state.conscious_workspace, query_embedding=memory_embedding, max_total_tokens=left_tokens)
        msgs = [("system", sysprompt)]
        msgs.extend(turns)

        comp_settings = CommonCompSettings(temperature=0.6, repeat_penalty=1.05, max_tokens=2000)
        ass, story = self.llm.completion_agentic(LlmPreset.Default, mcp_server_url, msgs, comp_settings=comp_settings)

        predicted_ai_emotion = EmotionalAxesModel(
            valence=1,
            affection=1,
            self_worth=1,
            trust=0.7,
            disgust=0,
            anxiety=0
        )

        res = {
            "sim_id": 0,
            "action_type": ActionType.Reply,
            "ai_reply_content": story,
            "simulated_user_reaction": "",
            "predicted_ai_emotion": predicted_ai_emotion,
            "intent_fulfillment_score": 1,
            "needs_fulfillment_score": 1,
            "cognitive_congruence_score": 1,
            "final_score": 1
        }

        return res

    @profile
    def _deliberate_and_select_reply(self) -> Optional[Dict[str, Any]]:
        """
        Simulates potential actions (Reply, Ignore), rates them using narrative-informed prompts,
        and selects the best one based on predicted outcomes. (Expectation generation happens in _execute_action).
        """
        self._log_mental_mechanism(self._deliberate_and_select_reply, MLevel.Mid, f"Use the workspace and the intentions to simulate options (internally, non-causal) and find option that best serves to fulfill intentions.")

        workspace_knoxels = self.current_state.conscious_workspace.to_list()
        workspace_content = self.current_state.conscious_workspace.get_story(self)

        context = self._get_context_from_latest_causal_events()
        context_events = context.get_story(self.config, 1024)

        current_emotions = self.current_state.state_emotions
        current_needs = self.current_state.state_needs
        current_cognition = self.current_state.state_cognition

        intentions = [k for k in workspace_knoxels if isinstance(k, Intention) and k.internal]  # Only internal goals influence action choice directly here
        intention_summary = "; ".join([f"'{i.content}' (Urgency: {i.urgency:.2f})" for i in intentions]) if intentions else "None"

        # --- Determine Importance & Simulation Count (Unchanged) ---
        stimulus_valence = 0.0
        if self.current_state.primary_stimulus:
            stim_feeling = Enumerable(self.all_features).where(
                lambda f: f.feature_type == FeatureType.Feeling and f.tick_id == self.current_state.tick_id and "stimulus" in f.content.lower()
            ).first_or_default()
            if stim_feeling and stim_feeling.affective_valence is not None:
                stimulus_valence = stim_feeling.affective_valence
        max_intention_urgency = max([i.urgency for i in intentions] + [0.0])
        importance_score = max(abs(stimulus_valence), max_intention_urgency)

        num_simulations = 1
        if self.stimulus_triage == StimulusTriage.Moderate:
            num_simulations = self.config.min_simulations_per_reply
            if importance_score > self.config.importance_threshold_more_sims:
                num_simulations = self.config.mid_simulations_per_reply
        elif self.stimulus_triage == StimulusTriage.Significant:
            num_simulations = self.config.mid_simulations_per_reply
            if importance_score > self.config.importance_threshold_more_sims:
                num_simulations = self.config.max_simulations_per_reply

        self._log_mental_mechanism(self._deliberate_and_select_reply, MLevel.Mid, f"Action deliberation importance: {importance_score:.2f}. Running {num_simulations} simulation(s) for Reply.")

        # --- Prepare Narratives for Prompts (Unchanged) ---
        ai_bhv_narr = self.get_narrative(NarrativeTypes.BehaviorActionSelection, self.config.companion_name)
        ai_self_narr = self.get_narrative(NarrativeTypes.SelfImage, self.config.companion_name)
        ai_psy_narr = self.get_narrative(NarrativeTypes.PsychologicalAnalysis, self.config.companion_name)
        ai_rel_narr = self.get_narrative(NarrativeTypes.Relations, self.config.companion_name)
        user_psy_narr = self.get_narrative(NarrativeTypes.PsychologicalAnalysis, self.config.user_name)

        # --- Candidate Actions & Ratings ---
        action_options = []

        # 1. Evaluate "Ignore" Action (Unchanged)
        if True:
            ignore_score = 0
            predicted_ignore_emotion = current_emotions.model_copy()
            if current_needs.connection < 0.4:
                predicted_ignore_emotion.anxiety = min(1.0, predicted_ignore_emotion.anxiety + 0.1)
                predicted_ignore_emotion.valence = max(-1.0, predicted_ignore_emotion.valence - 0.05)
                ignore_score -= 0.1
            elif current_emotions.anxiety > 0.7:
                predicted_ignore_emotion.anxiety = max(0.0, predicted_ignore_emotion.anxiety - 0.1)
                ignore_score += 0.1
            ignore_intent_fulfillment = 0.0
            for intent in intentions:
                if "ignore" in intent.content.lower() or "disengage" in intent.content.lower() or "calm down" in intent.content.lower():
                    ignore_intent_fulfillment = max(ignore_intent_fulfillment, intent.urgency * 0.8)
            ignore_emotion_score = 0.5 + (predicted_ignore_emotion.get_overall_valence() - current_emotions.get_overall_valence()) * 0.5 - (
                    predicted_ignore_emotion.anxiety - current_emotions.anxiety) * 0.3
            final_ignore_score = (ignore_intent_fulfillment * 0.6) + (ignore_emotion_score * 0.4)
            final_ignore_score = max(0.0, min(1.0, final_ignore_score))

            # action_options.append({
            #    "sim_id": "ignore_0",
            #    "action_type": ActionType.Ignore,
            #    "ai_reply_content": "[Decides to ignore/not respond externally]",
            #    "simulated_user_reaction": "N/A",
            #    "predicted_ai_emotion": predicted_ignore_emotion,
            #    "intent_fulfillment_score": ignore_intent_fulfillment,
            #    "final_score": final_ignore_score
            # })
            logging.info(f"Evaluated {ActionType.Ignore}: Score={final_ignore_score:.3f}, IntentFulfill={ignore_intent_fulfillment:.2f}, EmotionScore={ignore_emotion_score:.2f}")

        ai_reply_content = ""
        simulated_user_reaction = ""

        # 1. Seed the generation
        sysprompt = f"""You are an expert story writer. You write a endearing story about the interactions between a next-gen AI companion {self.config.companion_name} and their user {self.config.user_name}.
        The AI {self.config.companion_name} is very advances and has a cognitive architecture based on LIDA with functional self awareness and all aspects of human cognition, emotion and needs.
        Use dialogue in {QUOTE_START}quotes{QUOTE_START}, narration in *italics*, and thoughts in (parentheses).
        One reply only, 23 paragraphs, with about 50 % dialogue."""

        user_prompt = f"""
        **{self.config.companion_name}'s Character:** {self.config.universal_character_card}
        **{self.config.companion_name}'s Typical Behavior:** {ai_bhv_narr.content if ai_bhv_narr else 'Default behavior.'}
        **Current Emotional State:** {current_emotions}
        **Current Needs State:** {current_needs}
        **Current Cognitive State:** {current_cognition}
        **Relationship ({self.config.companion_name} -> {self.config.user_name}):** {ai_rel_narr.content if ai_rel_narr else 'Default relationship view.'}
        **Analysis of {self.config.user_name}:** {user_psy_narr.content if user_psy_narr else 'Default user analysis.'}
        **Active Internal Goals:** {intention_summary}
        """

        sys_tokens = get_token_count(sysprompt)
        user_tokens = get_token_count(user_prompt)
        left_tokens = context_size - (sys_tokens + user_tokens)

        memory_embedding = self._get_situational_embedding()
        assistant_prompt = self._build_conscious_content_queue(self.config, conscious_workspace_content=self.current_state.conscious_workspace, query_embedding=memory_embedding, max_total_tokens=left_tokens)
        assistant_prompt += f"\n{self.config.companion_name} says: {QUOTE_START}"

        msg_seed = [("system", sysprompt),
                    ("user", user_prompt),
                    ("assistant", assistant_prompt)]

        TEST = False
        if TEST:
            comp_settings = CommonCompSettings(temperature=0.7, repeat_penalty=1.05, max_tokens=1024, eval_only=True)
            self.llm.completion_text(LlmPreset.Default, msg_seed, comp_settings=comp_settings)

        # 2. Evaluate "Reply" Action (Monte Carlo Simulations - Unchanged core simulation logic)
        simulation_results = []
        for i in range(num_simulations):
            sim_id = f"reply_{i}"
            self._log_mental_mechanism(self._deliberate_and_select_reply, MLevel.Debug, f"--- Running Reply Simulation {sim_id} ---")

            msgs = copy.copy(msg_seed)
            if TEST:
                t = time.time()
                d = DialogActPool()
                seed = d.answer_humor.generation_prefix  # select_random_modalities(1)[0].generation_prefix
                dialoge_seed = f"\n{companion_name} says: {QUOTE_START}"
                msgs = copy.copy(msg_seed)

                msgs[2] = (msgs[2][0], msgs[2][1] + seed + dialoge_seed)
                content_buffer = []
                content_ai_buffer = []
                content_user_buffer = []
                output_buffer = [dialoge_seed]

                def extractor(new_token: str) -> Union[None, str]:
                    # EOS? try prompting new line of dialoge
                    utterances = []
                    if new_token is None:
                        accepted_output = ""
                    else:
                        accepted_output = new_token
                        output_buffer.append(new_token)
                        full = "".join(output_buffer)
                        tmpu = UtteranceExtractor.detect_all(full)
                        utterances.extend(tmpu)

                    prompt_seed = ""
                    if len(utterances) > 0 or new_token is None:
                        output_buffer.clear()
                        if len(content_ai_buffer) == len(content_user_buffer):
                            if len(utterances) > 0:
                                content_ai_buffer.append(utterances[0])
                                content_buffer.append(utterances[0])
                            prompt_seed = f"\n{self.config.user_name} says: {QUOTE_START}"
                        else:
                            if len(utterances) > 0:
                                content_user_buffer.append(utterances[0])
                                content_buffer.append(utterances[0])
                            prompt_seed = f"\n{self.config.companion_name} says: {QUOTE_START}"
                        output_buffer.append(prompt_seed)

                    if len(content_user_buffer) == 3:
                        return None
                    else:
                        return accepted_output + prompt_seed

                comp_settings = CommonCompSettings(temperature=0.7, repeat_penalty=1.05, max_tokens=1024, completion_callback=extractor)
                res = self.llm.completion_text(LlmPreset.Default, msgs, comp_settings=comp_settings)
                t2 = time.time()
                print(t2 - t)
                for i, msg in enumerate(content_buffer):
                    p = companion_name
                    if i % 2 == 1:
                        p = user_name
                    print(p + ": " + msg[:30])
                continue

            comp_settings = CommonCompSettings(temperature=0.7, repeat_penalty=1.05, max_tokens=1024)
            utts = []
            while True:
                res = f"{QUOTE_START}" + self.llm.completion_text(LlmPreset.Default, msgs, comp_settings=comp_settings)
                utts = UtteranceExtractor.detect_all(res)
                if len(utts) > 0:
                    break

            ai_reply_content = utts[0]

            tmp = []
            if len(utts) > 1:
                tmp.append(utts[1])
            else:
                tmp_assistant_prompt = assistant_prompt + ai_reply_content + f"\n{self.config.user_name} says: {QUOTE_START}"
                msgs = [("system", sysprompt),
                        ("user", user_prompt),
                        ("assistant", tmp_assistant_prompt)]

                while True:
                    comp_settings = CommonCompSettings(temperature=0.7, repeat_penalty=1.05, max_tokens=1024)
                    res = f"{QUOTE_START}" + self.llm.completion_text(LlmPreset.Default, msgs, comp_settings=comp_settings)
                    utts_sub = UtteranceExtractor.detect_all(res)
                    if len(utts_sub) > 0:
                        user_reply_content = utts_sub[0]
                        tmp.append(user_reply_content)
                        break

            simulated_user_reaction = "\n".join(tmp)

            self._log_mental_mechanism(
                self._deliberate_and_select_reply, MLevel.Low,
                f"Simulating Reply #{i}: AI says '{ai_reply_content[:70]}...'. "
                f"Predicts User will say '{simulated_user_reaction[:70]}...'"
            )
            # modifiers
            strong_emotions_summary = _verbalize_emotional_state(self.current_state.state_emotions)
            needs_summary_line, cognitive_style_summary = _verbalize_cognition_and_needs(self.current_state.state_cognition, self.current_state.state_needs)

            mental_state_prompt_block = f"""
            **{self.config.companion_name}'s Current State of Mind:**
            - **Strong Emotions:** {strong_emotions_summary}
            - **Needs:** {needs_summary_line}
            - **Cognitive Style:** {cognitive_style_summary}
            """

            # --- Rate Simulation with a Single, Holistic Call ---
            prompt_rate_action = f"""
            You are a 'Critic' codelet within a LIDA-inspired cognitive architecture, evaluating a potential action for the AI, {self.config.companion_name}.

            **Current Internal State (The "Now"):**
            - **Emotions:** {strong_emotions_summary}
            - **Pressing Needs:** {needs_summary_line}
            - **Cognitive State:** {mental_state_prompt_block} # Reuse the block we built for generation
            - **Active Internal Goals (Intentions):** {intention_summary}

            **Simulated Future (The "What If"):**
            - **AI's Proposed Action:** "{ai_reply_content}"
            - **Predicted User Reaction:** "{simulated_user_reaction}"

            **Task: Holistically evaluate this "What If" scenario from the perspective of the "Now".**
            Provide scores from 0.0 to 1.0 based on the following criteria:

            1.  **`intent_fulfillment`**: Does the action and reaction achieve the 'Active Internal Goals'?
            2.  **`needs_fulfillment`**: Does the action address the 'Pressing Needs'? (e.g., a personal reply helps 'connection').
            3.  **`predicted_emotional_impact`**: Analyze the exchange. Will it likely lead to a positive emotional shift (more joy, less anxiety)? A score of +1.0 is a huge positive shift, -1.0 a huge negative one, 0.0 is neutral.
            4.  **`cognitive_load`**: Based on the *current* Cognitive State, how difficult is this action to perform? Consider the length/complexity of the reply. Is it a simple, easy response, or does it require a lot of thought and emotional control?

            Output *only* a JSON object conforming to the `ActionRating` schema.
            """

            # Call the LLM to get the comprehensive rating
            class ActionRating(BaseModel):
                """A comprehensive rating for a simulated action, considering multiple dimensions."""
                overall_reasoning: str = Field(description="A brief, holistic justification for the scores provided.")
                intent_fulfillment: float = Field(..., ge=0.0, le=1.0, description="How well this action and its predicted outcome fulfill the AI's active internal goals.")
                needs_fulfillment: float = Field(..., ge=0.0, le=1.0, description="How well this action fulfills the AI's most pressing needs (e.g., connection, relevance).")
                predicted_emotional_impact: float = Field(..., ge=-1.0, le=1.0, description="The predicted net change in emotional state. Positive is better (more joy, less anxiety), negative is worse.")
                cognitive_load: float = Field(..., ge=0.0, le=1.0, description="The cognitive effort required for this action. 0.0 is effortless, 1.0 is highly demanding (long, complex, emotionally difficult).")

            action_rating_result = self._call_llm(prompt_rate_action, output_schema=ActionRating, temperature=0.2)
            # --- Calculate the Final Score using Procedural Logic ---

            self._log_mental_mechanism(
                self._deliberate_and_select_reply, MLevel.Low,
                f"Critic rated Simulation #{i}: Intent={action_rating_result.intent_fulfillment:.2f}, "
                f"Needs={action_rating_result.needs_fulfillment:.2f}, "
                f"EmotionImpact={action_rating_result.predicted_emotional_impact:+.2f}. "
                f"Reason: {action_rating_result.overall_reasoning}"
            )

            # a) Get the scores from the LLM
            intent_fulfillment_score = action_rating_result.intent_fulfillment
            needs_fulfillment_score = action_rating_result.needs_fulfillment
            emotion_score = (action_rating_result.predicted_emotional_impact + 1.0) / 2.0  # Normalize [-1, 1] to [0, 1]
            cognitive_load = action_rating_result.cognitive_load

            # b) Calculate Cognitive Congruence Score (Procedural Logic)
            # This score reflects if the action is *wise* given the current resources.
            # It's high if the action is a good fit for the current state.
            # We start with a perfect score and apply penalties.
            cognitive_congruence_score = 1.0

            # Penalize taking on high-load actions when willpower is low.
            if current_cognition.willpower < 0.4 and cognitive_load > 0.6:
                # The penalty is greater the lower the willpower.
                penalty = (cognitive_load - 0.5) * (1.0 - current_cognition.willpower)
                cognitive_congruence_score -= penalty
                self._log_mental_mechanism(..., MLevel.Debug, f"Sim {sim_id}: Penalizing for high cognitive load ({cognitive_load:.2f}) with low willpower ({current_cognition.willpower:.2f}). Penalty: {penalty:.2f}")

            cognitive_congruence_score = max(0.0, cognitive_congruence_score)

            # c) Combine scores with clear weights
            final_sim_score = (intent_fulfillment_score * 0.40) \
                              + (needs_fulfillment_score * 0.30) \
                              + (emotion_score * 0.15) \
                              + (cognitive_congruence_score * 0.15)  # Give congruence more weight

            final_sim_score = max(0.0, min(1.0, final_sim_score))

            # d) To get the predicted future emotional state, we can run a quick appraisal on the impact score
            # This is less critical now but can be useful for logging. A simple approximation is fine.
            predicted_emotion_delta = EmotionalAxesModelDelta(valence=action_rating_result.predicted_emotional_impact)
            predicted_ai_emotion = current_emotions + predicted_emotion_delta

            simulation_results.append({
                "sim_id": sim_id,
                "action_type": ActionType.Reply,
                "ai_reply_content": ai_reply_content,
                "simulated_user_reaction": simulated_user_reaction,
                "predicted_ai_emotion": predicted_ai_emotion,
                "intent_fulfillment_score": intent_fulfillment_score,
                "needs_fulfillment_score": needs_fulfillment_score,
                "cognitive_congruence_score": cognitive_congruence_score,
                "final_score": final_sim_score
            })

            self._log_mental_mechanism(self._deliberate_and_select_reply, MLevel.Low, f"--- Finished Reply Simulation {sim_id}: Score={final_sim_score:.3f}, Intent={intent_fulfillment_score:.2f}, Needs={needs_fulfillment_score:.2f}, Emo={emotion_score:.2f}, CogCongruence={cognitive_congruence_score:.2f} ---")
            self._log_mental_mechanism(self._deliberate_and_select_reply, MLevel.Debug, f"Rating Reasoning: {action_rating_result.overall_reasoning}")

        # Add successful simulations to options
        action_options.extend(simulation_results)

        # --- Select Best Action (Unchanged) ---
        action_options.sort(key=lambda x: x["final_score"], reverse=True)
        best_action_details = action_options[0]

        self._log_mental_mechanism(self._deliberate_and_select_reply, MLevel.Low, f"Selected Action: {best_action_details['action_type']} (Score: {best_action_details['final_score']:.3f})")
        if best_action_details['action_type'] == ActionType.Reply:
            self._log_mental_mechanism(self._deliberate_and_select_reply, MLevel.Mid, f"Winning Content: {best_action_details['ai_reply_content'][:100]}...")
            self._log_mental_mechanism(self._deliberate_and_select_reply, MLevel.Mid, f"Predicted User Reaction for chosen action: {best_action_details['simulated_user_reaction'][:100]}...")

        self.current_state.action_simulations = action_options
        return best_action_details

    @profile
    def _generate_expectations_for_action(self, action_knoxel: Action, action_details: Dict[str, Any]):
        """
        Generates Intention knoxels (internal=False) representing expectations
        based on the executed action and its predicted user reaction.
        """
        self._log_mental_mechanism(self._generate_expectations_for_action, MLevel.Debug, "Generate world / user expectations for selected action.")

        ai_reply = action_details["ai_reply_content"]
        predicted_reaction = action_details["simulated_user_reaction"]
        current_emotions = self.current_state.state_emotions if self.current_state else EmotionalAxesModel()

        class ExpectationSchema(BaseModel):
            content: str = Field(..., description=f"A brief description of what {self.config.companion_name} now expects {self.config.user_name} to say or do next.")
            affective_valence: float = Field(..., ge=-1.0, le=1.0, description=f"The valence (-1 to 1) {self.config.companion_name} *desires* or anticipates if this expectation is met.")
            urgency: float = Field(default=0.3, ge=0.0, le=1.0, description="Initial urgency (0-1) of this expectation.")
            # Proximal/Distal could be added here as a field later if needed

        # Dynamically create the list schema
        ExpectationListSchema = create_model(
            'ExpectationListSchema',
            expectations=(List[ExpectationSchema], Field(..., description=f"List of {self.config.expectation_generation_count} generated expectations."))
        )

        prompt = f"""
        Character: {self.config.companion_name}
        Current Emotional State: {current_emotions}
        {self.config.companion_name} just said: "{ai_reply}"
        Predicted {self.config.user_name}'s reaction: "{predicted_reaction}"

        Task: Based on what {self.config.companion_name} said and the predicted reaction, generate {self.config.expectation_generation_count} specific, plausible things {self.config.companion_name} might now *expect* {self.config.user_name} to say or do in the near future (next turn or two).
        For each expectation:
        - `content`: Describe the expected user action/utterance briefly.
        - `affective_valence`: Estimate the emotional valence (-1 to 1) {self.config.companion_name} would likely experience *if* this expectation is met. Consider her goals and personality (e.g., expecting agreement might be positive, expecting a question might be neutral or slightly positive if curious, expecting criticism negative).
        - `urgency`: Assign a low-to-moderate initial urgency (e.g., 0.1 to 0.5).

        Output *only* a JSON object matching the schema with a single key "expectations" containing the list.
        Example: {{ "expectations": [ {{ "content": "Expect {user_name} to agree with the cloud sheep idea", "affective_valence": 0.6, "urgency": 0.4 }}, ... ] }}
        """

        generated_expectations_result: ExpectationListSchema = self._call_llm(prompt, output_schema=ExpectationListSchema, temperature=0.6)

        created_expectation_ids = []
        if generated_expectations_result and isinstance(generated_expectations_result, ExpectationListSchema):
            self._log_mental_mechanism(self._generate_short_term_intentions, MLevel.Debug, f"Generated {len(generated_expectations_result.expectations)} potential expectations for Action {action_knoxel.id}.")

            # duplicates -> fulfill old ones and replace with new
            for exp_data in generated_expectations_result.expectations:
                emb_new = self.llm.get_embedding(exp_data.content)
                for existing_intention in self.all_intentions:
                    if not existing_intention.internal:
                        emb_existing = self.llm.get_embedding(existing_intention.content)
                        sim = cosine_pair(emb_new, emb_existing)
                        if sim < vector_same_threshold:
                            existing_intention.fulfilment = 1

            for exp_data in generated_expectations_result.expectations:
                # Calculate initial salience based on valence and urgency
                incentive_salience = max(0.1, min(0.9, (abs(exp_data.affective_valence) * 0.5 + exp_data.urgency * 0.5)))
                expectation_knoxel = Intention(
                    content=exp_data.content,
                    affective_valence=exp_data.affective_valence,
                    urgency=exp_data.urgency,
                    incentive_salience=incentive_salience,
                    internal=False,  # Mark as external expectation
                    fulfilment=0.0,
                    originating_action_id=action_knoxel.id,  # Link back to the action
                    tick_id=self._get_current_tick_id()  # Created in this tick
                )
                self.add_knoxel(expectation_knoxel)  # Adds to memory, generates embedding
                created_expectation_ids.append(expectation_knoxel.id)

                self._log_mental_mechanism(self._generate_short_term_intentions, MLevel.Debug,
                                           f"Created Expectation {expectation_knoxel.id} for Action {action_knoxel.id}: '{exp_data.content}' (Desired V: {exp_data.affective_valence:.2f}, Urg: {exp_data.urgency:.2f})")
        else:
            logging.warning(f"Failed to generate expectations for Action {action_knoxel.id} via LLM.")

        return created_expectation_ids

    @profile
    def _deliberate_to_initiate_conversation(self) -> Dict[str, Any]:
        """
        Deliberates on and formulates a proactive conversation starter.
        This reuses the reply deliberation logic, as the goal is still to generate
        an optimal utterance based on the current context and internal state.
        """
        self._log_mental_mechanism(self._deliberate_to_initiate_conversation, MLevel.Mid, "Deliberating on how to proactively start a conversation...")

        # We can directly reuse the reply simulation, as it's designed to generate
        # the best possible utterance given the conscious workspace content.
        # The workspace will contain the 'LowNeedTrigger' or 'EngagementOpportunity'
        # stimulus, which will guide the generation process.
        reply_details = self._deliberate_and_select_reply()

        # Crucially, we override the action_type to correctly label the final action.
        reply_details["action_type"] = ActionType.InitiateUserConversation

        self._log_mental_mechanism(
            self._deliberate_to_initiate_conversation, MLevel.Mid,
            f"Formulated conversation starter: '{reply_details['ai_reply_content'][:100]}...'"
        )
        return reply_details

    @profile
    def _deliberate_to_sleep(self) -> Dict[str, Any]:
        """
        Determines the duration for a sleep state based on current needs and emotions.
        """
        self._log_mental_mechanism(self._deliberate_to_sleep, MLevel.Mid, "Deliberating on entering a sleep state...")

        current_willpower = self.current_state.state_cognition.willpower
        current_anxiety = self.current_state.state_emotions.anxiety

        # Determine sleep duration with a simple heuristic.
        # Base sleep is 4 hours. Low willpower adds time, high anxiety reduces it (restless sleep).
        base_sleep_hours = 4.0
        willpower_modifier = (0.5 - current_willpower) * 4  # Max +2 hours for 0 willpower
        anxiety_modifier = (current_anxiety - 0.5) * -2  # Max -1 hour for 1.0 anxiety

        sleep_duration_hours = base_sleep_hours + willpower_modifier + anxiety_modifier
        # Clamp the duration to a reasonable range (e.g., 1 to 8 hours)
        sleep_duration_hours = max(1.0, min(8.0, sleep_duration_hours))

        # The content of the action is a human-readable description of the internal decision.
        action_content = f"Entering a restorative sleep state for approximately {sleep_duration_hours:.1f} hours to recover willpower."

        # Predict the emotional state after sleeping: generally positive.
        # A full reset might be too strong, so we just move towards a healthy baseline.
        predicted_emotion = self.current_state.state_emotions.model_copy(deep=True)
        predicted_emotion.decay_to_baseline(decay_factor=0.8)  # Strong decay towards baseline
        predicted_emotion.anxiety = max(0.0, predicted_emotion.anxiety - 0.5)  # Significantly reduce anxiety

        return {
            "action_type": ActionType.Sleep,
            "ai_reply_content": action_content,
            "simulated_user_reaction": "N/A",
            "predicted_ai_emotion": predicted_emotion,
            "final_score": 1.0,  # If this action was chosen, we are 100% confident
            "sleep_duration_hours": sleep_duration_hours  # Pass this to the Shell
        }

    @profile
    def _deliberate_for_internal_contemplation(self) -> Dict[str, Any]:
        """
        Initiates a Tree of Thoughts (ToT) process to deeply analyze the current
        situation in the conscious workspace. The result is a summary of the
        contemplation, which becomes the content of the Action knoxel.
        """
        self._log_mental_mechanism(self._deliberate_for_internal_contemplation, MLevel.High, "Initiating internal contemplation (Tree of Thoughts)...")

        # The conscious workspace provides the context for the contemplation.
        workspace_story = self.current_state.conscious_workspace.get_story(self)
        last_user_message = self.current_state.primary_stimulus.content

        # The "knowledge" for ToT can be a summary of relevant narratives.
        # This is a good place to consolidate the AI's self-image and goals.
        self_narr = self.get_narrative(NarrativeTypes.SelfImage, self.config.companion_name)
        goals_narr = self.get_narrative(NarrativeTypes.GoalsIntentions, self.config.companion_name)
        knowledge_summary = f"My Self-Image: {self_narr.content}\nMy Goals: {goals_narr.content}"

        # --- Generate the Tree of Thoughts ---
        # We create a TreeOfThought instance and run the generation.
        # The max_depth can be adjusted based on the importance of the stimulus.
        tot = TreeOfThought(self.llm)
        # For a significant stimulus, use a deeper tree.
        max_depth = 6 if self.stimulus_triage == StimulusTriage.Significant else 4

        thought_chain_str = tot.generate_tree_of_thoughts_str(
            context=workspace_story,
            last_user_message=last_user_message,
            knowledge=knowledge_summary,
            max_depth=max_depth
        )

        # --- Summarize the Contemplation ---
        # The thought chain itself is too verbose. We ask the LLM to summarize its conclusion.
        prompt_summary = f"""
        The AI companion {self.config.companion_name} just completed a deep internal contemplation (a Tree of Thoughts process) about the current situation.

        Full Thought Process:
        ---
        {thought_chain_str}
        ---

        Task: Summarize the key insight or conclusion from this thought process in a single, concise sentence. This summary will be recorded as the AI's internal action.
        Example: "I've realized I need to be more direct about my feelings."
        """

        contemplation_summary = self._call_llm(prompt_summary, temperature=0.3, max_tokens=128)
        if not contemplation_summary:
            contemplation_summary = "Completed a period of deep thought."

        self._log_mental_mechanism(
            self._deliberate_for_internal_contemplation, MLevel.High,
            f"Contemplation concluded. Insight: {contemplation_summary}"
        )

        # The emotional state after contemplation is typically calmer and more resolved.
        predicted_emotion = self.current_state.state_emotions.model_copy(deep=True)
        predicted_emotion.anxiety *= 0.7  # Reduce anxiety
        predicted_emotion.valence += 0.1  # Slightly increase valence from clarity

        return {
            "action_type": ActionType.InitiateInternalContemplation,
            "ai_reply_content": contemplation_summary,  # The "content" is the insight.
            "simulated_user_reaction": "N/A",
            "predicted_ai_emotion": predicted_emotion,
            "final_score": 1.0,
        }

    # In Ghost class
    @profile
    def _deliberate_for_tool_call(self) -> Dict[str, Any]:
        """
        Deliberates on and formulates a proactive, standalone tool call based on
        an internal goal or an idea from the Engagement Strategist.
        """
        self._log_mental_mechanism(self._deliberate_for_tool_call, MLevel.Mid, "Deliberating on a proactive tool call...")

        # The stimulus content for a ToolCall action will be the EngagementIdea JSON.
        idea_json = self.current_state.primary_stimulus.content
        idea = EngagementIdea.model_validate_json(idea_json)

        # The action content is the tool call parameters from the idea.
        action_content = idea.action_content

        self._log_mental_mechanism(
            self._deliberate_for_tool_call, MLevel.Mid,
            f"Formulated proactive tool call based on idea: {idea.thought_process}"
        )

        possible_tool_call, tn = self._check_possible_tool_call()
        if possible_tool_call:
            experience_content = f"I could call the tool '{tn}' for this."
            subjective_feature = Feature(
                content=experience_content,
                feature_type=FeatureType.Thought,
                affective_valence=self.current_state.state_emotions.get_overall_valence(),
                interlocus=-1, causal=True, source=self.config.companion_name
            )
            self.add_knoxel(subjective_feature)
            self.current_state.conscious_workspace.add(subjective_feature)
            self.current_state.subjective_experience_tool = subjective_feature
        else:
            experience_content = f"Hmmm, looks like there isn't a good tool for this."
            subjective_feature = Feature(
                content=experience_content,
                feature_type=FeatureType.Thought,
                affective_valence=self.current_state.state_emotions.get_overall_valence(),
                interlocus=-1, causal=True, source=self.config.companion_name
            )
            self.add_knoxel(subjective_feature)

            predicted_emotion = self.current_state.state_emotions.model_copy(deep=True)
            predicted_emotion.anxiety *= 1.1  # increase slightly
            predicted_emotion.valence -= 0.1  # decrease slightly

            res = {
                "sim_id": 0,
                "action_type": ActionType.Reply,
                "ai_reply_content": experience_content,
                "simulated_user_reaction": "",
                "predicted_ai_emotion": predicted_emotion,
                "intent_fulfillment_score": 1,
                "needs_fulfillment_score": 1,
                "cognitive_congruence_score": 1,
                "final_score": 1
            }
            return res

        sysprompt = character_card_assistant
        sys_tokens = get_token_count(sysprompt)
        left_tokens = context_size - sys_tokens

        memory_embedding = self._get_situational_embedding()
        turns = self._build_conscious_content_queue_assistant(self.config, conscious_workspace_content=self.current_state.conscious_workspace, query_embedding=memory_embedding, max_total_tokens=left_tokens)
        msgs = [("system", sysprompt)]
        msgs.extend(turns)

        comp_settings = CommonCompSettings(temperature=0.6, repeat_penalty=1.05, max_tokens=2000)
        ass, story = self.llm.completion_agentic(LlmPreset.Default, mcp_server_url, msgs, comp_settings=comp_settings)

        predicted_emotion = self.current_state.state_emotions.model_copy(deep=True)
        predicted_emotion.anxiety *= 0.7  # Reduce anxiety
        predicted_emotion.valence += 0.1  # Slightly increase valence from clarity

        res = {
            "sim_id": 0,
            "action_type": ActionType.Reply,
            "ai_reply_content": story,
            "simulated_user_reaction": "",
            "predicted_ai_emotion": predicted_emotion,
            "intent_fulfillment_score": 1,
            "needs_fulfillment_score": 1,
            "cognitive_congruence_score": 1,
            "final_score": 1
        }
        return res

    @profile
    def _execute_action(self, chosen_action_type: ActionType) -> Action:
        """
        Dispatcher that selects the appropriate deliberation method based on the
        chosen action type, then makes the resulting action plan causal.
        """
        # --- 1. Deliberation Phase: Call the correct method ---
        if chosen_action_type == ActionType.Reply:
            self.current_state.selected_action_details = self._deliberate_and_select_reply()
        elif chosen_action_type == ActionType.ToolCallAndReply:
            self.current_state.selected_action_details = self._perform_tool_call_and_reply()
        elif chosen_action_type == ActionType.InitiateUserConversation:
            self.current_state.selected_action_details = self._deliberate_to_initiate_conversation()
        elif chosen_action_type == ActionType.InitiateInternalContemplation:
            self.current_state.selected_action_details = self._deliberate_for_internal_contemplation()
        elif chosen_action_type == ActionType.Sleep:
            self.current_state.selected_action_details = self._deliberate_to_sleep()
        elif chosen_action_type == ActionType.ToolCall:
            self.current_state.selected_action_details = self._deliberate_for_tool_call()
        elif chosen_action_type == ActionType.Ignore:
            # Ignore has no deliberation; we create a minimal action plan.
            self.current_state.selected_action_details = {
                "action_type": ActionType.Ignore,
                "ai_reply_content": f"Decided to ignore stimulus.",
                "simulated_user_reaction": "N/A",
                "predicted_ai_emotion": self.current_state.state_emotions,
                "final_score": 1.0
            }
        else:
            raise ValueError(f"Unknown action type passed to _execute_action: {chosen_action_type}")

        self._log_mental_mechanism(self._execute_action, MLevel.Mid, "Making selected action plan causal and sending to world")

        action_details = self.current_state.selected_action_details
        action_type = action_details["action_type"]
        content = action_details["ai_reply_content"]

        # --- 2. Causality Phase: Create Knoxels ---
        action_knoxel = Action(
            action_type=action_type,
            content=content,
            tick_id=self._get_current_tick_id()
        )
        self.add_knoxel(action_knoxel)
        self.current_state.selected_action_knoxel = action_knoxel

        # Generate expectations only for actions that interact with the user
        if action_type in [ActionType.Reply, ActionType.ToolCallAndReply, ActionType.InitiateUserConversation]:
            generated_expectation_ids = self._generate_expectations_for_action(action_knoxel, action_details)
            if generated_expectation_ids:
                action_knoxel.generated_expectation_ids = generated_expectation_ids

        # Create a causal Feature knoxel for ALL actions to record them in the story
        predicted_emotion_state = action_details.get("predicted_ai_emotion", self.current_state.state_emotions)

        # Determine if the action is primarily internal or external for the feature
        is_external = action_type in [
            ActionType.Reply, ActionType.ToolCallAndReply, ActionType.InitiateUserConversation, ActionType.ToolCall
        ]
        interlocus_val = 1 if is_external else -1

        action_feature_content = ""
        if action_type == ActionType.Reply:
            action_feature_content = content  # The dialogue itself is the action
        elif action_type == ActionType.InitiateUserConversation:
            action_feature_content = content
        elif action_type == ActionType.ToolCall or action_type == ActionType.ToolCallAndReply:
            action_feature_content = f"Used a tool with parameters: {content}"
        elif action_type == ActionType.InitiateInternalContemplation:
            action_feature_content = f"Thought deeply and concluded: '{content}'"
        elif action_type == ActionType.Ignore:
            action_feature_content = f"Chose to remain silent and observe."
        elif action_type == ActionType.Sleep:
            action_feature_content = content  # e.g., "Entering a sleep state..."

        # Use the appropriate feature type
        feature_type = FeatureType.Dialogue if action_type in [ActionType.Reply, ActionType.InitiateUserConversation] else FeatureType.Action

        # Create the single, unified feature
        causal_feature = Feature(
            source=self.config.companion_name,
            content=action_feature_content,
            feature_type=feature_type,
            affective_valence=predicted_emotion_state.get_overall_valence(),
            interlocus=interlocus_val,
            causal=True
        )
        self.add_knoxel(causal_feature)

        self._log_mental_mechanism(self._execute_action, MLevel.Mid, f"Executed Action {action_knoxel.id} ({action_knoxel.action_type.value}), Output: {content[:100]}...")
        return action_knoxel

    @profile
    def _execute_action_old(self, chosen_action_type: ActionType) -> Action:
        """
        Creates the causal Action knoxel, generates associated expectations,
        adds them to memory, and returns the AI's content if it's an external action.
        """
        if chosen_action_type == ActionType.Reply:
            self.current_state.selected_action_details = self._deliberate_and_select_reply()
        elif chosen_action_type == ActionType.ToolCallAndReply:
            self.current_state.selected_action_details = self._perform_tool_call_and_reply()
        elif chosen_action_type == ActionType.Ignore:
            pass

        self._log_mental_mechanism(self._execute_action, MLevel.Mid, "Make selected action causal and send to world")

        action_details = self.current_state.selected_action_details
        action_type = action_details["action_type"]
        content = action_details["ai_reply_content"]
        self.simulated_reply = action_details["simulated_user_reaction"]

        # --- Create and Store Action Knoxel ---
        action_knoxel = Action(
            action_type=action_type,
            content=content,
            # generated_expectation_ids will be filled below
            tick_id=self._get_current_tick_id()
        )
        # Add the action knoxel *first* to get its ID
        self.add_knoxel(action_knoxel)
        self.current_state.selected_action_knoxel = action_knoxel

        # --- Generate Expectations (if applicable) ---
        generated_expectation_ids = []
        if action_type == ActionType.Reply or action_type == ActionType.ToolCallAndReply:
            generated_expectation_ids = self._generate_expectations_for_action(action_knoxel, action_details)
            if generated_expectation_ids:
                # Update the action knoxel with the IDs of expectations it generated
                action_knoxel.generated_expectation_ids = generated_expectation_ids
                logging.info(f"Linked {len(generated_expectation_ids)} expectations to Action {action_knoxel.id}.")

            # --- Create Causal Feature (Unchanged) ---
            predicted_emotion_state = action_details.get("predicted_ai_emotion", self.current_state.state_emotions)

            action_feature = Feature(
                content=f"{action_type} predicted emotional state",
                feature_type=FeatureType.Action,
                affective_valence=predicted_emotion_state.get_overall_valence(),
                interlocus=1 if action_type == ActionType.Reply else -1,
                causal=True, source=self.config.companion_name
            )
            self.add_knoxel(action_feature)

            reply_feature = Feature(
                source=self.config.companion_name,
                content=content,
                feature_type=FeatureType.Dialogue,
                affective_valence=predicted_emotion_state.get_overall_valence(),
                interlocus=1,
                causal=True,
            )
            self.add_knoxel(reply_feature)
        elif action_type == ActionType.Ignore:
            action_feature = Feature(
                content=f"{action_type}",
                feature_type=FeatureType.Action,
                affective_valence=-0.5,
                interlocus=-1,
                causal=True, source=self.config.companion_name
            )
            self.add_knoxel(action_feature)

            reply_feature = Feature(
                source=self.config.companion_name,
                content=f"*Ignores {self.config.user_name}*",
                feature_type=FeatureType.Dialogue,
                affective_valence=-0.5,
                interlocus=1,
                causal=True,
            )
            self.add_knoxel(reply_feature)

        self._log_mental_mechanism(self._execute_action, MLevel.Mid, f"Executed Action {action_knoxel.id}, Output: {content}")
        return action_knoxel

    @profile
    def _build_conscious_content_queue_assistant(
            self,
            cfg: GhostConfig,
            query_embedding: List[float],
            conscious_workspace_content: KnoxelList,
            max_total_tokens: int = 1500,  # Adjusted default
            recent_budget_ratio: float = 0.66,  # Give more to recent direct features
            topical_budget_ratio: float = 0.33,
            temporal_budget_ratio: float = 0
    ) -> List[Tuple[str, str]]:
        """
        Creates an optimized story prompt by balancing recent causal features,
        relevant topical cluster events, and relevant temporal summaries.
        """
        # --- Token Budget Allocation ---
        # Ensure ratios sum to 1, adjust if not (though here they do

        selected_knoxels_for_story: Dict[int, KnoxelBase] = {}  # Use dict to ensure unique knoxels by ID
        selected_knoxels_for_story[self.current_state.subjective_experience_tool.id] = self.current_state.subjective_experience_tool

        total_ratio = recent_budget_ratio + topical_budget_ratio + temporal_budget_ratio
        if not math.isclose(total_ratio, 1.0):
            # logging.warning(f"Budget ratios do not sum to 1.0 (sum: {total_ratio}). Normalizing.")
            recent_budget_ratio /= total_ratio
            topical_budget_ratio /= total_ratio
            temporal_budget_ratio /= total_ratio

        recent_token_budget = int(max_total_tokens * recent_budget_ratio)
        topical_token_budget = int(max_total_tokens * topical_budget_ratio)
        temporal_token_budget = int(max_total_tokens * temporal_budget_ratio)

        # --- 1. Select Most Recent Causal Features ---
        logging.debug(f"Recent causal feature budget: {recent_token_budget} tokens.")

        dialouge = Enumerable(self.all_features) \
            .where(lambda x: x.causal) \
            .where(lambda x: FeatureType.from_stimulus(x.feature_type)) \
            .where(lambda x: x.tick_id != self.current_tick_id) \
            .order_by(lambda x: x.timestamp_world_begin) \
            .to_list()

        current_tick = Enumerable(self.all_features) \
            .where(lambda x: x.causal) \
            .where(lambda x: x.tick_id == self.current_tick_id) \
            .order_by(lambda x: x.timestamp_world_begin) \
            .to_list()

        recent_causal_features = dialouge + current_tick
        current_recent_tokens = 0
        for feature in recent_causal_features[::-1]:
            tokens = get_token_count(feature)
            if current_recent_tokens + tokens <= recent_token_budget:
                if feature.id not in selected_knoxels_for_story:
                    selected_knoxels_for_story[feature.id] = feature
                    current_recent_tokens += tokens
            else:
                break
        logging.info(f"Selected {len(selected_knoxels_for_story)} recent causal features, using {current_recent_tokens} tokens.")

        # --- 2. Select Relevant Topical Cluster Events ---
        logging.debug(f"Topical cluster event budget: {topical_token_budget} tokens.")
        all_topical_clusters = [
            k for k in self.all_episodic_memories  # Assuming all_episodic_memories holds MemoryClusterKnoxels
            if k.cluster_type == ClusterType.Topical and k.embedding and k.included_event_ids
        ]

        if all_topical_clusters:
            # Rank topical clusters by relevance
            ranked_topical_clusters = sorted(
                all_topical_clusters,
                key=lambda c: cosine_distance(np.array(query_embedding), np.array(c.embedding))
            )  # Low distance = high relevance

            current_topical_tokens = 0
            for cluster in ranked_topical_clusters:
                if current_topical_tokens >= topical_token_budget:
                    break

                event_ids_in_cluster = [int(eid) for eid in cluster.included_event_ids.split(',') if eid]
                events_to_add_from_cluster: List[KnoxelBase] = []
                tokens_for_this_cluster_events = 0

                # Get events from this cluster, preferring those not already selected
                # and sort them chronologically within the cluster
                cluster_event_knoxels = sorted(
                    [self.get_knoxel_by_id(eid) for eid in event_ids_in_cluster if self.get_knoxel_by_id(eid)],
                    key=lambda e: (e.tick_id, e.id)
                )

                for event in cluster_event_knoxels:
                    if event.id not in selected_knoxels_for_story:
                        tokens = get_token_count(event)
                        if current_topical_tokens + tokens_for_this_cluster_events + tokens <= topical_token_budget:
                            events_to_add_from_cluster.append(event)
                            tokens_for_this_cluster_events += tokens
                        else:  # Not enough budget for this specific event from cluster
                            break

                # Add the collected events from this cluster
                for event in events_to_add_from_cluster:
                    selected_knoxels_for_story[event.id] = event
                current_topical_tokens += tokens_for_this_cluster_events
            logging.info(f"Added events from topical clusters, using {current_topical_tokens} tokens.")

        # --- Combine, Sort, and Generate Story ---
        final_knoxels_for_story = sorted(
            selected_knoxels_for_story.values(),
            key=lambda k: k.timestamp_world_begin  # Chronological by creation
        )

        logger.info("Logging current story elements:")
        for k in final_knoxels_for_story:
            logger.info(k.get_story_element(self).replace("\n", "\\n"))

        story_list = KnoxelList(final_knoxels_for_story)
        final_story_str = story_list.get_story(self, max_tokens=max_total_tokens)  # Pass the original max_total_tokens

        self._log_mental_mechanism(self._deliberate_and_select_reply, MLevel.Debug,
                                   f"Generated causal story with {len(final_knoxels_for_story)} knoxels. Final length approx {len(final_story_str) / (cfg.context_events_similarity_max_tokens / 512 * 3.5) :.0f} tokens.")  # Rough estimate

        turns = []
        for k in final_knoxels_for_story:
            if isinstance(k, Feature):
                if FeatureType.from_stimulus(k.feature_type):
                    if k.source == companion_name:
                        turns.append(("assistant", k.content))
                    elif k.source == user_name:
                        turns.append(("user", k.content))
                    else:
                        turns.append(("user", f"{k.source}: {k.content}"))
        return turns

    @profile
    def _build_conscious_content_queue(
            self,
            cfg: GhostConfig,
            query_embedding: List[float],
            conscious_workspace_content: KnoxelList,
            max_total_tokens: int = 1500,  # Adjusted default
            recent_budget_ratio: float = 0.50,  # Give more to recent direct features
            topical_budget_ratio: float = 0.25,
            temporal_budget_ratio: float = 0.25
    ) -> str:
        """
        Creates an optimized story prompt by balancing recent causal features,
        relevant topical cluster events, and relevant temporal summaries.
        """
        # --- Token Budget Allocation ---
        # Ensure ratios sum to 1, adjust if not (though here they do

        workspace_size = get_token_count(conscious_workspace_content.get_story(self))
        max_total_tokens -= workspace_size

        selected_knoxels_for_story: Dict[int, KnoxelBase] = {}  # Use dict to ensure unique knoxels by ID
        for f in conscious_workspace_content._list:
            selected_knoxels_for_story[f.id] = f

        total_ratio = recent_budget_ratio + topical_budget_ratio + temporal_budget_ratio
        if not math.isclose(total_ratio, 1.0):
            logging.warning(f"Budget ratios do not sum to 1.0 (sum: {total_ratio}). Normalizing.")
            recent_budget_ratio /= total_ratio
            topical_budget_ratio /= total_ratio
            temporal_budget_ratio /= total_ratio

        recent_token_budget = int(max_total_tokens * recent_budget_ratio)
        topical_token_budget = int(max_total_tokens * topical_budget_ratio)
        temporal_token_budget = int(max_total_tokens * temporal_budget_ratio)

        # --- 1. Select Most Recent Causal Features ---
        logging.debug(f"Recent causal feature budget: {recent_token_budget} tokens.")

        dialouge = Enumerable(self.all_features) \
            .where(lambda x: x.causal) \
            .where(lambda x: FeatureType.from_stimulus(x.feature_type)) \
            .where(lambda x: x.tick_id != self.current_tick_id) \
            .order_by(lambda x: x.timestamp_world_begin) \
            .to_list()

        current_tick = Enumerable(self.all_features) \
            .where(lambda x: x.causal) \
            .where(lambda x: x.tick_id == self.current_tick_id) \
            .order_by(lambda x: x.timestamp_world_begin) \
            .to_list()

        recent_causal_features = dialouge + current_tick
        current_recent_tokens = 0
        for feature in recent_causal_features[::-1]:
            tokens = get_token_count(feature)
            if current_recent_tokens + tokens <= recent_token_budget:
                if feature.id not in selected_knoxels_for_story:
                    selected_knoxels_for_story[feature.id] = feature
                    current_recent_tokens += tokens
            else:
                break
        logging.info(f"Selected {len(selected_knoxels_for_story)} recent causal features, using {current_recent_tokens} tokens.")

        # --- 2. Select Relevant Topical Cluster Events ---
        logging.debug(f"Topical cluster event budget: {topical_token_budget} tokens.")
        all_topical_clusters = [
            k for k in self.all_episodic_memories  # Assuming all_episodic_memories holds MemoryClusterKnoxels
            if k.cluster_type == ClusterType.Topical and k.embedding and k.included_event_ids
        ]

        if all_topical_clusters:
            # Rank topical clusters by relevance
            ranked_topical_clusters = sorted(
                all_topical_clusters,
                key=lambda c: cosine_distance(np.array(query_embedding), np.array(c.embedding))
            )  # Low distance = high relevance

            current_topical_tokens = 0
            for cluster in ranked_topical_clusters:
                if current_topical_tokens >= topical_token_budget:
                    break

                event_ids_in_cluster = [int(eid) for eid in cluster.included_event_ids.split(',') if eid]
                events_to_add_from_cluster: List[KnoxelBase] = []
                tokens_for_this_cluster_events = 0

                # Get events from this cluster, preferring those not already selected
                # and sort them chronologically within the cluster
                cluster_event_knoxels = sorted(
                    [self.get_knoxel_by_id(eid) for eid in event_ids_in_cluster if self.get_knoxel_by_id(eid)],
                    key=lambda e: (e.tick_id, e.id)
                )

                for event in cluster_event_knoxels:
                    if event.id not in selected_knoxels_for_story:
                        tokens = get_token_count(event)
                        if current_topical_tokens + tokens_for_this_cluster_events + tokens <= topical_token_budget:
                            events_to_add_from_cluster.append(event)
                            tokens_for_this_cluster_events += tokens
                        else:  # Not enough budget for this specific event from cluster
                            break

                # Add the collected events from this cluster
                for event in events_to_add_from_cluster:
                    selected_knoxels_for_story[event.id] = event
                current_topical_tokens += tokens_for_this_cluster_events
            logging.info(f"Added events from topical clusters, using {current_topical_tokens} tokens.")

        # --- 3. Select Relevant Temporal Summaries ---
        logging.debug(f"Temporal summary budget: {temporal_token_budget} tokens.")
        all_temporal_summaries = [
            k for k in self.all_episodic_memories
            if k.cluster_type == ClusterType.Temporal and k.embedding and k.content
        ]

        if all_temporal_summaries:
            # Rank temporal summaries
            ranked_temporal_summaries = sorted(
                all_temporal_summaries,
                key=lambda s: cosine_distance(np.array(query_embedding), np.array(s.embedding)) if s.embedding else 1.0
            )

            current_temporal_tokens = 0
            for summary in ranked_temporal_summaries:
                if summary.id not in selected_knoxels_for_story:  # Don't add if somehow already there
                    tokens = get_token_count(summary)
                    if current_temporal_tokens + tokens <= temporal_token_budget:
                        selected_knoxels_for_story[summary.id] = summary
                        current_temporal_tokens += tokens
                    else:
                        break
            logging.info(f"Selected temporal summaries, using {current_temporal_tokens} tokens.")

        # --- Combine, Sort, and Generate Story ---
        final_knoxels_for_story = sorted(
            selected_knoxels_for_story.values(),
            key=lambda k: k.timestamp_world_begin  # Chronological by creation
        )

        logger.info("Logging current story elements:")
        for k in final_knoxels_for_story:
            logger.info(k.get_story_element(self).replace("\n", "\\n"))

        story_list = KnoxelList(final_knoxels_for_story)
        final_story_str = story_list.get_story(cfg, max_tokens=max_total_tokens)  # Pass the original max_total_tokens

        self._log_mental_mechanism(self._deliberate_and_select_reply, MLevel.Debug,
                                   f"Generated causal story with {len(final_knoxels_for_story)} knoxels. Final length approx {len(final_story_str) / (cfg.context_events_similarity_max_tokens / 512 * 3.5) :.0f} tokens.")  # Rough estimate
        return final_story_str
